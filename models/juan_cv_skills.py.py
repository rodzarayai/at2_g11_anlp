# -*- coding: utf-8 -*-
"""NER

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ngit0N8p6kxpaVqtnO-UE2qWgXfu8GgK

# NER Model

In this notebook, we train a NER model to identify organisations and skills based on CVs

## Steps in the notebook


1. Setting the environment
2. Loading and exploring datasets     
3. Defining NER model for entity identification
4. Defining NER model for skills identification
5. Preparing the embedding

## Setting the environment and loading datasets

Install necessary libraries and packages
"""

!pip install spacy
!python -m spacy download en_core_web_sm
!pip install pdfminer.six
!pip install docx2txt

# Importing essential libraries and functions
import spacy
nlp = spacy.load('en_core_web_sm')

import pandas as pd
import numpy as np
import re
import nltk
from nltk.corpus import stopwords
from numpy import array
import seaborn as sns
import matplotlib.pyplot as plt
import time
import json
from collections import Counter
from collections import defaultdict
import torch
import docx2txt
import random


from tqdm import tqdm
from transformers import pipeline, AutoTokenizer, AutoModel
from subprocess import list2cmdline
from pdfminer.high_level import extract_text
from sklearn.metrics.pairwise import cosine_similarity
from spacy.matcher import Matcher
from tensorflow.keras.preprocessing.text import one_hot, Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Activation, Dropout, Dense, Flatten, GlobalMaxPooling1D, Embedding, Conv1D, LSTM
from tensorflow.keras.layers import Flatten, GlobalMaxPooling1D, Embedding, Conv1D, LSTM
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer

# Mounting google drive

from google.colab import drive
drive.mount('/content/drive')

from google.colab import files
uploaded = files.upload()

from google.colab import files
uploaded = files.upload()

"""## Exploring datasets"""

# Explore resume file

text = pd.read_csv("Resume.csv")

# Dataset exploration

text.shape

text.head(5)

#Explore skills file

data = []
with open("jz_skill_patterns.jsonl", 'r') as f:
    for line in f:
        data.append(json.loads(line))


text2 = pd.DataFrame(data)

text2.shape

text2.head(5)

text2.tail(5)

"""## Defining NER model for entity identification

Parts of this model were developed in reference to: https://stackoverflow.com/questions/72986264/company-name-extraction-with-bert-base-ner-easy-way-to-know-which-words-relate

Certain debugging was done using Claude.
"""

#Defining NER model for entity identification

start = time.time()
model_checkpoint = "xlm-roberta-large-finetuned-conll03-english"
token_classifier = pipeline(
    "token-classification", model=model_checkpoint, aggregation_strategy="simple"
)

def text_extraction(file):
    try:
        df = pd.read_csv(file)
        text = ' '.join(df.astype(str).values.flatten())
        return text.replace('\t', ' ')
    except Exception as e:
        print(f"Error reading CSV file: {e}")
        return None

def preprocess_text(text):
    # Remove HTML tags
    text = re.sub(r'<[^>]+>', ' ', text)
    # Replace newlines and multiple spaces with a single space
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

def extract_organizations(text):
    # Use regex to find potential company names
    company_patterns = [
        r'(?:^|\s)(?:at|for|with)\s+((?:[A-Z][a-z]+\s?)+(?:Inc\.?|LLC|Ltd\.?|Corporation|Corp\.?|Company|Co\.?)?)',
        r'(?:^|\s)((?:[A-Z][a-z]+\s?)+(?:Inc\.?|LLC|Ltd\.?|Corporation|Corp\.?|Company|Co\.?))',
        r'Company Name\s*[－-]\s*([^,]+)'
    ]

    potential_orgs = []
    for pattern in company_patterns:
        potential_orgs.extend(re.findall(pattern, text))

    # Use the model for additional entity recognition
    classifier_results = token_classifier(text)
    model_orgs = [item["word"] for item in classifier_results if item["entity_group"] == "ORG"]

    # Combine results
    all_orgs = list(set(potential_orgs + model_orgs))

    return post_process_orgs(all_orgs)

def post_process_orgs(orgs):
    # Remove duplicates and sort
    orgs = sorted(set(orgs))
    # Remove generic terms misclassified as organizations
    blacklist = {'hr', 'human resources', 'payroll', 'assistant', 'intern', 'coordinator', 'manager', 'company name'}
    orgs = [org for org in orgs if org.lower() not in blacklist]
    return orgs

def analyze_resume(file):
    extracted_text = text_extraction(file)
    if extracted_text is None:
        return

    preprocessed_text = preprocess_text(extracted_text)
    organizations = extract_organizations(preprocessed_text)

    print("Extracted Organizations:")
    for i, org in enumerate(organizations, 1):
        print(f"{i}. {org}")

    print(f"\nTotal unique organizations extracted: {len(organizations)}")

csv_file = "Resume.csv"
analyze_resume(csv_file)

end = time.time()
print(f"\nExecution time: {round((end - start), 2)} seconds")

"""## Defining NER2 for skills identification

"""

#Defining NER model for skills identification

start = time.time()
model_checkpoint = "xlm-roberta-large-finetuned-conll03-english"
token_classifier = pipeline(
    "token-classification", model=model_checkpoint, aggregation_strategy="simple"
)

def text_extraction(file):
    """
    To extract texts from pdf, word, or csv files
    """
    if file.endswith(".pdf"):
        return extract_text(file)
    elif file.endswith(".docx"):
        resume_text = docx2txt.process(file)
        if resume_text:
            return resume_text.replace('\t', ' ')
        return None
    elif file.endswith(".csv"):
        try:
            df = pd.read_csv(file)
            text = ' '.join(df.astype(str).values.flatten())
            return text.replace('\t', ' ')
        except Exception as e:
            print(f"Error reading file: {e}")
            return None
    else:
        return None

def load_custom_entities(jsonl_file):
    custom_entities = []
    with open(jsonl_file, 'r') as file:
        for line in file:
            data = json.loads(line)
            pattern = data.get('pattern', [])
            skill = " ".join([item.get('LOWER', '') for item in pattern]).strip()
            if skill:
                custom_entities.append(skill)
    return custom_entities

def preprocess_text(text):
    text = text.lower()
    text = re.sub(r'[^\w\s]', ' ', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text

def find_skills(text, skills):
    found_skills = []
    for skill in skills:
        if skill in text:
            found_skills.append(skill)
    return found_skills

def custom_entity_extraction(file, custom_entities):
    extracted_text = text_extraction(file)
    if not extracted_text:
        print("No text extracted from the file.")
        return

    preprocessed_text = preprocess_text(extracted_text)

    # Find skills in the preprocessed text
    found_skills = find_skills(preprocessed_text, custom_entities)

    # Use the model for additional entity recognition
    classifier_results = token_classifier(extracted_text)

    # Combine results
    all_entities = set(found_skills + [item['word'].lower() for item in classifier_results if item['word'].lower() in custom_entities])

    # Sort entities
    sorted_entities = sorted(all_entities, key=lambda x: (-len(x), x))

    print("Extracted Skills:")
    for skill in sorted_entities:
        print(f"  {skill}")

    print(f"\nTotal unique skills extracted: {len(sorted_entities)}")

# Load custom entities from JSONL file
jsonl_file = "jz_skill_patterns.jsonl"
custom_entities = load_custom_entities(jsonl_file)

# Apply model
custom_entity_extraction("Resume.csv", custom_entities)

end = time.time()
print(f"\nExecution time: {round((end - start), 2)} seconds")

"""## Defining and testing models with embedding - Organisations

We will now develop the model to be used for our application which will use embeddings to build flexibility and robustness.

We will split our Resume.csv data into a train and validation split so that we can evalaute the performance of our embeddings.

Claude AI was used to identify ways to optimize the code as the original model was taking several hours to process.

Creating word embeddings from two models
"""

# Embedding for entity model

start = time.time()
model_name = "xlm-roberta-large"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name)

def split_data(file, test_size=0.2):
    df = pd.read_csv(file)
    train_df, val_df = train_test_split(df, test_size=test_size, random_state=42)
    return train_df, val_df

def text_extraction(df):
    text = ' '.join(df.astype(str).values.flatten())
    return text.replace('\t', ' ')

def preprocess_text(text):
    text = re.sub(r'<[^>]+>', ' ', text)
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

def get_embedding(text):
    inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=512, padding=True)
    with torch.no_grad():
        outputs = model(**inputs)
    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()

def extract_potential_orgs(text):
    company_patterns = [
        r'(?:^|\s)(?:at|for|with)\s+((?:[A-Z][a-z]+\s?)+(?:Inc\.?|LLC|Ltd\.?|Corporation|Corp\.?)?)',
        r'(?:^|\s)((?:[A-Z][a-z]+\s?)+(?:Inc\.?|LLC|Ltd\.?|Corporation|Corp\.?))',
        r'Company Name\s*[－-]\s*([^,]+)'
    ]

    potential_orgs = []
    for pattern in company_patterns:
        potential_orgs.extend(re.findall(pattern, text))

    return list(set(potential_orgs))

def is_likely_org(org_name, org_embedding, known_org_embeddings, threshold=0.8):
    similarities = cosine_similarity([org_embedding], known_org_embeddings)[0]
    return np.max(similarities) > threshold

def post_process_orgs(orgs):
    orgs = sorted(set(orgs))
    blacklist = {'hr', 'human resources', 'payroll', 'assistant', 'intern', 'coordinator', 'manager', 'company name', 'Zip Code Phone Number Email Address Date Employer Co', 's', 'io', 'wide', 'Youth', 'You',
                 'Year Achievement Award Navy Unit Commendation Navy Meritorious Unit Co', 'Work History Company Name Certifications Inorganic Chemistry Organic Chemistry Materials Science Nanomaterials Unit Operations Fluid Mechanics Thermodynamics Transport Phenomenon Process Co',
                 'Success', 'City', 'Accountant', 'Accounting', 'Accounts', 'Accreditation', 'Accruals', 'Act', 'Action', 'Active Directory', 'Administration', 'Administrators',
                 'Advertising', 'Advising', 'Advisor', 'Assistant', 'Agile', 'Attending daily fit meetings; evaluating the status and functionality of samples based on updates before approving for production. - Creating/updating line sheets',
                 'Business Development', 'City', 'Closing', 'Community', 'Compliance', 'Executive', 'Interest', 'Leadership', 'Microsoft Excel', 'Operations' , 'Qualifications', 'State', 'Substitute Teacher'
                 }
    orgs = [org for org in orgs if org.lower() not in blacklist]
    return orgs

def analyze_resume(df, set_name):
    extracted_text = text_extraction(df)
    preprocessed_text = preprocess_text(extracted_text)
    potential_orgs = extract_potential_orgs(preprocessed_text)

    known_orgs = ["Google", "Microsoft", "Apple", "Amazon", "Facebook", "IBM", "Intel"]
    known_org_embeddings = [get_embedding(org) for org in known_orgs]

    confirmed_orgs = []
    for org in potential_orgs:
        org_embedding = get_embedding(org)
        if is_likely_org(org, org_embedding, known_org_embeddings):
            confirmed_orgs.append(org)

    confirmed_orgs = post_process_orgs(confirmed_orgs)

    print(f"\nExtracted Organizations ({set_name}):")
    for i, org in enumerate(confirmed_orgs, 1):
        print(f"{i}. {org}")

    print(f"\nTotal unique organizations extracted ({set_name}): {len(confirmed_orgs)}")

csv_file = "Resume.csv"
train_df, val_df = split_data(csv_file)

print(f"Training set size: {len(train_df)}")
print(f"Validation set size: {len(val_df)}")

analyze_resume(train_df, "Training Set")
analyze_resume(val_df, "Validation Set")

end = time.time()
print(f"\nExecution time: {round((end - start), 2)} seconds")

# We will evaluate our model by referring to a sample of organisations from our validation dataset

random.seed(42)
random.sample(val_df, k=100)

"""## Defining and testing models with embedding - Skills"""

start = time.time()

model_name = "distilroberta-base"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name)

def split_data(file, test_size=0.2):
    df = pd.read_csv(file)
    train_df, val_df = train_test_split(df, test_size=test_size, random_state=42)
    return train_df, val_df

def load_custom_entities(jsonl_file):
    custom_entities = []
    with open(jsonl_file, 'r') as file:
        for line in file:
            data = json.loads(line)
            pattern = data.get('pattern', [])
            skill = " ".join([item.get('LOWER', '') for item in pattern]).strip()
            if skill:
                custom_entities.append(skill)
    return custom_entities

def preprocess_text(text):
    text = text.lower()
    text = re.sub(r'[^\w\s]', ' ', text)
    return re.sub(r'\s+', ' ', text).strip()

@torch.no_grad()
def get_embedding(text):
    inputs = tokenizer(text, padding=True, truncation=True, return_tensors="pt", max_length=128)
    outputs = model(**inputs)
    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()

def find_skills(text_embedding, skill_embeddings, threshold=0.8):
    similarities = cosine_similarity([text_embedding], skill_embeddings)[0]
    return [i for i, sim in enumerate(similarities) if sim > threshold]

def extract_ngrams(text, n_range=(1, 4)):
    words = text.split()
    ngrams = []
    for n in range(n_range[0], n_range[1] + 1):
        ngrams.extend([' '.join(words[i:i+n]) for i in range(len(words)-n+1)])
    return ngrams

def custom_entity_extraction(df, custom_entities, skill_embeddings, set_name):
    all_skills = []
    all_texts = df['Resume_str'].apply(preprocess_text).tolist()

    # TF-IDF vectorization
    tfidf = TfidfVectorizer(ngram_range=(1, 4), stop_words='english')
    tfidf_matrix = tfidf.fit_transform(all_texts)
    feature_names = tfidf.get_feature_names_out()

    print(f"Processing resumes for {set_name}...")
    for idx, text in tqdm(enumerate(all_texts), total=len(all_texts)):
        text_embedding = get_embedding(text)

        # Embedding-based skill matching
        skills_indices = find_skills(text_embedding, skill_embeddings)
        resume_skills = [custom_entities[i] for i in skills_indices]

        # N-gram based skill matching
        ngrams = extract_ngrams(text)
        ngram_skills = [ng for ng in ngrams if ng in custom_entities]

        # TF-IDF based skill extraction
        tfidf_skills = [feature_names[i] for i in tfidf_matrix[idx].nonzero()[1] if feature_names[i] in custom_entities]

        # Combine, deduplicate, and randomly sample skills
        combined_skills = list(set(resume_skills + ngram_skills + tfidf_skills))
        sampled_skills = random.sample(combined_skills, min(len(combined_skills), 10))
        all_skills.extend(sampled_skills)

    skill_counts = pd.Series(all_skills).value_counts()

    print(f"\nTop 20 Extracted Skills ({set_name}):")
    for skill, count in skill_counts.head(20).items():
        print(f"  {skill} (Count: {count})")

    print(f"\nTotal unique skills extracted ({set_name}): {len(skill_counts)}")
    return skill_counts

# Main execution
if __name__ == "__main__":
    jsonl_file = "jz_skill_patterns.jsonl"
    custom_entities = load_custom_entities(jsonl_file)
    skill_embeddings = np.array([get_embedding(skill) for skill in custom_entities])

    csv_file = "Resume.csv"
    train_df, val_df = split_data(csv_file)

    print(f"Training set size: {len(train_df)}")
    print(f"Validation set size: {len(val_df)}")

    train_skills = custom_entity_extraction(train_df, custom_entities, skill_embeddings, "Training Set")
    val_skills = custom_entity_extraction(val_df, custom_entities, skill_embeddings, "Validation Set")

    train_skill_set = set(train_skills.index)
    val_skill_set = set(val_skills.index)

    print("\nComparison:")
    print(f"Skills in training set: {len(train_skill_set)}")
    print(f"Skills in validation set: {len(val_skill_set)}")
    print(f"Skills in both sets: {len(train_skill_set.intersection(val_skill_set))}")
    print(f"Skills unique to training set: {len(train_skill_set - val_skill_set)}")
    print(f"Skills unique to validation set: {len(val_skill_set - train_skill_set)}")

    print("\nExamples of skills unique to training set:")
    print(list(train_skill_set - val_skill_set)[:5])
    print("\nExamples of skills unique to validation set:")
    print(list(val_skill_set - train_skill_set)[:5])

    end = time.time()
    print(f"\nTotal execution time: {round((end - start), 2)} seconds")