{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "kSbPNLvQXRJx",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kSbPNLvQXRJx",
        "outputId": "743f21d1-e284-4aef-e77b-b104b417e22d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting contractions\n",
            "  Downloading contractions-0.1.73-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting textsearch>=0.0.21 (from contractions)\n",
            "  Downloading textsearch-0.0.24-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting anyascii (from textsearch>=0.0.21->contractions)\n",
            "  Downloading anyascii-0.3.2-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting pyahocorasick (from textsearch>=0.0.21->contractions)\n",
            "  Downloading pyahocorasick-2.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (13 kB)\n",
            "Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
            "Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyahocorasick-2.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (110 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.7/110.7 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.1.0 textsearch-0.0.24\n"
          ]
        }
      ],
      "source": [
        "#pip install contractions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "a3f0f87e-66c8-4559-b45f-e93646af4c30",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3f0f87e-66c8-4559-b45f-e93646af4c30",
        "outputId": "871104d2-118e-42c9-8e71-7f2ff0f878cf",
        "tags": []
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/rodzaraya/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /Users/rodzaraya/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /Users/rodzaraya/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "\n",
        "#Text processing and cleaning\n",
        "import contractions # To include english contractions\n",
        "import re #regex\n",
        "import string #used to include punctuation during text processing\n",
        "from collections import Counter #count strings in texts\n",
        "\n",
        "#Natural Language Tool Kit NLK package\n",
        "import nltk\n",
        "from nltk.corpus import stopwords #Stopwords\n",
        "\n",
        "import spacy\n",
        "from spacy.matcher import Matcher\n",
        "\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b41ca7a-7c1d-4808-9023-7b90b3356db6",
      "metadata": {
        "id": "6b41ca7a-7c1d-4808-9023-7b90b3356db6",
        "tags": []
      },
      "source": [
        "All the cell above must be in the aux_func.py file as auxiliary functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "ce12ea3f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading 'en_core_web_sm' model...\n",
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.8/12.8 MB 6.7 MB/s eta 0:00:00\n",
            "Requirement already satisfied: spacy<3.8.0,>=3.7.2 in /Users/rodzaraya/.pyenv/versions/3.9.13/lib/python3.9/site-packages (from en-core-web-sm==3.7.1) (3.7.5)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/rodzaraya/.pyenv/versions/3.9.13/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /Users/rodzaraya/.pyenv/versions/3.9.13/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: jinja2 in /Users/rodzaraya/.pyenv/versions/3.9.13/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/rodzaraya/.pyenv/versions/3.9.13/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /Users/rodzaraya/.pyenv/versions/3.9.13/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /Users/rodzaraya/.pyenv/versions/3.9.13/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.1)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/rodzaraya/.pyenv/versions/3.9.13/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/rodzaraya/.pyenv/versions/3.9.13/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.5)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/rodzaraya/.pyenv/versions/3.9.13/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/rodzaraya/.pyenv/versions/3.9.13/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/rodzaraya/.pyenv/versions/3.9.13/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /Users/rodzaraya/.pyenv/versions/3.9.13/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: setuptools in /Users/rodzaraya/.pyenv/versions/3.9.13/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (58.1.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Users/rodzaraya/.pyenv/versions/3.9.13/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.9.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/rodzaraya/.pyenv/versions/3.9.13/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/rodzaraya/.pyenv/versions/3.9.13/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/rodzaraya/.pyenv/versions/3.9.13/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /Users/rodzaraya/.pyenv/versions/3.9.13/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.12.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/rodzaraya/.pyenv/versions/3.9.13/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: language-data>=1.2 in /Users/rodzaraya/.pyenv/versions/3.9.13/lib/python3.9/site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/rodzaraya/.pyenv/versions/3.9.13/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /Users/rodzaraya/.pyenv/versions/3.9.13/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /Users/rodzaraya/.pyenv/versions/3.9.13/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.23.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/rodzaraya/.pyenv/versions/3.9.13/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.8.30)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/rodzaraya/.pyenv/versions/3.9.13/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/rodzaraya/.pyenv/versions/3.9.13/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/rodzaraya/.pyenv/versions/3.9.13/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.2.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/rodzaraya/.pyenv/versions/3.9.13/lib/python3.9/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/rodzaraya/.pyenv/versions/3.9.13/lib/python3.9/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: rich>=10.11.0 in /Users/rodzaraya/.pyenv/versions/3.9.13/lib/python3.9/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.8.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /Users/rodzaraya/.pyenv/versions/3.9.13/lib/python3.9/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /Users/rodzaraya/.pyenv/versions/3.9.13/lib/python3.9/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /Users/rodzaraya/.pyenv/versions/3.9.13/lib/python3.9/site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.0.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /Users/rodzaraya/.pyenv/versions/3.9.13/lib/python3.9/site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.19.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /Users/rodzaraya/.pyenv/versions/3.9.13/lib/python3.9/site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /Users/rodzaraya/.pyenv/versions/3.9.13/lib/python3.9/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/rodzaraya/.pyenv/versions/3.9.13/lib/python3.9/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/rodzaraya/.pyenv/versions/3.9.13/lib/python3.9/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: wrapt in /Users/rodzaraya/.pyenv/versions/3.9.13/lib/python3.9/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /Users/rodzaraya/.pyenv/versions/3.9.13/lib/python3.9/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
            "Installing collected packages: en-core-web-sm\n",
            "Successfully installed en-core-web-sm-3.7.1\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: You are using pip version 22.0.4; however, version 24.2 is available.\n",
            "You should consider upgrading via the '/Users/rodzaraya/.pyenv/versions/3.9.13/bin/python -m pip install --upgrade pip' command.\n"
          ]
        }
      ],
      "source": [
        "import subprocess\n",
        "\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "except OSError:\n",
        "    # If the model is not found, download it\n",
        "    print(\"Downloading 'en_core_web_sm' model...\")\n",
        "    subprocess.run([\"python\", \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"])\n",
        "    nlp = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "db716670-e047-48cb-97c7-9d5eae90bea9",
      "metadata": {
        "id": "db716670-e047-48cb-97c7-9d5eae90bea9",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModel, AutoTokenizer\n",
        "import torch\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "8e9a3883",
      "metadata": {},
      "outputs": [],
      "source": [
        "#pip install scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "n3Bm4s1oXLa3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n3Bm4s1oXLa3",
        "outputId": "5f0c72d9-646a-4f77-d34d-0f74efaea124"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "bb4cc0b5-0149-4a77-b536-b472246bdf03",
      "metadata": {
        "id": "bb4cc0b5-0149-4a77-b536-b472246bdf03",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def load_skills_from_json(skills_file):\n",
        "    skills_list = []\n",
        "    with open(skills_file, 'r') as f:\n",
        "        data = json.load(f)  # Load the entire JSON file\n",
        "        for skill in data.keys():  # The keys at the top level are the skill names\n",
        "            skills_list.append(skill)  # Add the skill to the list\n",
        "    return skills_list\n",
        "\n",
        "\n",
        "#=======================================Processing class\n",
        "class TextPreprocessor:\n",
        "    def __init__(self, processing_mode='none', custom_punctuation=None, custom_stopwords=None, sentence_analysis=False):\n",
        "        \"\"\"\n",
        "            Initialization considers Custom punctuation, Stop words, and Lemmatizer or Stemmer.\n",
        "            Updates custom punctuation and custom stop words set with additional ones if provided.\n",
        "            The processing mode to standardise variants can be choose between none, Stem and Lemma. Each mode is stored in a different\n",
        "            column of the dataframe.\n",
        "            Sentence analysis parameter is used to keep the punctuation symbols required for sentence analysis.\n",
        "\n",
        "            Parameters:\n",
        "            - processing_mode: String to decide whether to use 'lemma', 'stem', or 'none' for text processing.\n",
        "            - custom_punctuation: Additional punctuation characters to remove from text.\n",
        "            - custom_stopwords: Additional stopwords to remove from text.\n",
        "            - sentence_analysis: Boolean indicating sentence analysis cleaning steps. This mode will keep the punctuation symbols.\n",
        "\n",
        "            \"\"\"\n",
        "\n",
        "        self.punctuation = string.punctuation #Init with all punctuation characters\n",
        "\n",
        "        if custom_punctuation:\n",
        "            self.punctuation += custom_punctuation #add custom punctuation\n",
        "\n",
        "        self.stop_words = set(stopwords.words('english'))\n",
        "        if custom_stopwords:\n",
        "            self.stop_words.update(custom_stopwords) #add custom stopwords\n",
        "\n",
        "        # Determine which text processing mode to use\n",
        "        self.processing_mode = processing_mode.lower()\n",
        "\n",
        "        # Set the sentence analysis mode\n",
        "        self.sentence_analysis = sentence_analysis\n",
        "\n",
        "        #Set the variant standardization mode\n",
        "        if self.processing_mode == 'lemma':\n",
        "            self.lemmatizer = WordNetLemmatizer()\n",
        "        elif self.processing_mode == 'stem':\n",
        "            self.stemmer = PorterStemmer()\n",
        "\n",
        "    # Expand contractions using the contractions library\n",
        "    def expand_contractions(self, text):\n",
        "        return contractions.fix(text)\n",
        "\n",
        "    # Split hyphenated words into separate words, like phone numbers or radio fm, age, etc.\n",
        "    def split_hyphenated_words(self, text):\n",
        "        return re.sub(r'-', ' ', text)\n",
        "\n",
        "    def remove_punctuation(self, text):\n",
        "        return ''.join([char for char in text if char not in self.punctuation])\n",
        "\n",
        "    def add_space_after_parenthesis(self, text):\n",
        "        return re.sub(r'\\)', ') ', text)\n",
        "\n",
        "    def to_lowercase(self, text):\n",
        "        return text.lower()\n",
        "\n",
        "    def remove_stopwords(self, text):\n",
        "        words = word_tokenize(text)\n",
        "        return ' '.join([word for word in words if word not in self.stop_words])\n",
        "\n",
        "    def remove_extra_whitespace(self, text):\n",
        "        return re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    def stem_words(self, text):\n",
        "        words = word_tokenize(text)\n",
        "        return ' '.join([self.stemmer.stem(word) for word in words])\n",
        "\n",
        "    def lemmatize_words(self, text):\n",
        "        words = word_tokenize(text)\n",
        "        return ' '.join([self.lemmatizer.lemmatize(word) for word in words])\n",
        "\n",
        "    # Order matters\n",
        "    def preprocess(self, text):\n",
        "        text = self.expand_contractions(text)\n",
        "        text = self.split_hyphenated_words(text)\n",
        "        text = self.add_space_after_parenthesis(text)\n",
        "\n",
        "        #In case we need to analyse sentences, we will need the punctuations\n",
        "        if not self.sentence_analysis:\n",
        "            text = self.remove_punctuation(text)\n",
        "        text = self.to_lowercase(text)\n",
        "        #The stopwords are removed if the users wants to standardise variants.\n",
        "        #If none is selected, the ouput will just perform previous cleaning steps\n",
        "        if self.processing_mode != 'none':\n",
        "            text = self.remove_stopwords(text)\n",
        "\n",
        "        text = self.remove_extra_whitespace(text)\n",
        "\n",
        "        #Select the processing mode for variants\n",
        "        if self.processing_mode == 'lemma':\n",
        "            text = self.lemmatize_words(text)\n",
        "        elif self.processing_mode == 'stem':\n",
        "            text = self.stem_words(text)\n",
        "\n",
        "        return text\n",
        "\n",
        "    #Apply preprocessing steps to daframe and create a column base on the processing mode\n",
        "    def preprocess_dataframe(self, df, column_name):\n",
        "        if not self.sentence_analysis:\n",
        "            if self.processing_mode == 'lemma':\n",
        "                df[f'{column_name}_processed_lemma'] = df[column_name].apply(self.preprocess)\n",
        "            elif self.processing_mode == 'stem':\n",
        "                df[f'{column_name}_processed_stem'] = df[column_name].apply(self.preprocess)\n",
        "            else:  # If 'none', apply preprocessing without lemma or stem\n",
        "                df[f'{column_name}_processed_cleaned'] = df[column_name].apply(self.preprocess)\n",
        "        else: # Add different processed columns for sentences\n",
        "            if self.processing_mode == 'lemma':\n",
        "                df[f'{column_name}_processed_lemma_sent'] = df[column_name].apply(self.preprocess)\n",
        "            elif self.processing_mode == 'stem':\n",
        "                df[f'{column_name}_processed_stem_sent'] = df[column_name].apply(self.preprocess)\n",
        "            else:  # If 'none', apply preprocessing without lemma or stem\n",
        "                df[f'{column_name}_processed_cleaned_sent'] = df[column_name].apply(self.preprocess)\n",
        "        return df\n",
        "\n",
        "#=============================== GET THE EMBEDDINGS\n",
        "\n",
        "def get_embeddings(text, model_name):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    inputs = tokenizer(str(text), return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
        "    outputs = model(**inputs)\n",
        "    embeddings = outputs.last_hidden_state.mean(dim=1).detach().to(\"cpu\").numpy()\n",
        "    return embeddings\n",
        "\n",
        "\n",
        "#============================== GET THE SKILLS\n",
        "\n",
        "# Define a function to apply the matcher and find skills in the text\n",
        "def find_skills(text):\n",
        "    doc = nlp(text)\n",
        "    matches = matcher(doc)\n",
        "    skills = set()  # To store found skills\n",
        "    for match_id, start, end in matches:\n",
        "        skill = doc[start:end].text\n",
        "        skills.add(skill)\n",
        "    return skills\n",
        "\n",
        "\n",
        "def find_top_skills(job_desc_embedding, skill_embeddings, skills_list, threshold=0.55):\n",
        "    # Ensure that job_desc_embedding is 2D before passing to cosine_similarity\n",
        "    #job_desc_embedding = np.expand_dims(job_desc_embedding, axis=0)  # Make it 2D\n",
        "    similarities = cosine_similarity(job_desc_embedding, skill_embeddings).flatten()\n",
        "\n",
        "    # Find all skills with similarity scores above the threshold\n",
        "    above_threshold_indices = [i for i, score in enumerate(similarities) if score >= threshold]\n",
        "\n",
        "    # Get the skills and scores for those above the threshold\n",
        "    top_skills = [skills_list[i] for i in above_threshold_indices]\n",
        "    top_scores = [similarities[i] for i in above_threshold_indices]\n",
        "\n",
        "    # Return both the skills and their similarity scores\n",
        "    return list(zip(top_skills, top_scores))\n",
        "\n",
        "#@st.cache_resource\n",
        "def load_model(model_name):\n",
        "\n",
        "\n",
        "    # Select device (MPS for Mac, CUDA for NVIDIA GPUs, CPU as a fallback)\n",
        "    if torch.backends.mps.is_available():\n",
        "        device = torch.device(\"mps\")\n",
        "    elif torch.cuda.is_available():\n",
        "        device = torch.device(\"cuda\")\n",
        "    else:\n",
        "        device = torch.device(\"cpu\")\n",
        "\n",
        "    model = AutoModel.from_pretrained(model_name)\n",
        "    model.to(device)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    return model, tokenizer, device\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "619daf25-68ac-486d-87fb-4ef2153648ab",
      "metadata": {
        "id": "619daf25-68ac-486d-87fb-4ef2153648ab"
      },
      "source": [
        "# LOAD FILES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "b8a7c74c-16b1-4963-b4dc-7e92b2dc0c99",
      "metadata": {
        "id": "b8a7c74c-16b1-4963-b4dc-7e92b2dc0c99",
        "tags": []
      },
      "outputs": [],
      "source": [
        "#Load the files\n",
        "\n",
        "with open('../data/job_desc_embeddings_skills.pkl', 'rb') as f:\n",
        "#with open('/content/drive/MyDrive/AT2/data/job_desc_embeddings_skills.pkl', 'rb') as f: #using in colab\n",
        "    skill_embeddings = pickle.load(f)\n",
        "    skill_embeddings = np.squeeze(skill_embeddings, axis=1)\n",
        "\n",
        "with open('../data/job_desc_embeddings.pkl', 'rb') as f:\n",
        "#with open('/content/drive/MyDrive/AT2/data/job_desc_embeddings.pkl', 'rb') as f:\n",
        "    job_desc_embeddings = pickle.load(f)\n",
        "\n",
        "\n",
        "# Load patterns from the JSONL file\n",
        "skills_patterns = []\n",
        "with open('../data/jz_skill_patterns.jsonl', 'r') as f:\n",
        "#with open('/content/drive/MyDrive/AT2/data/jz_skill_patterns.jsonl', 'r') as f:\n",
        "    for line in f:\n",
        "        skills_patterns.append(json.loads(line))\n",
        "\n",
        "skills_list = load_skills_from_json('../data/skills.json')\n",
        "#skills_list = load_skills_from_json('/content/drive/MyDrive/AT2/data/skills.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "75608df6-23f5-4b82-a6bb-48d4b4a8867d",
      "metadata": {
        "id": "75608df6-23f5-4b82-a6bb-48d4b4a8867d",
        "tags": []
      },
      "outputs": [],
      "source": [
        "\n",
        "matcher = Matcher(nlp.vocab)\n",
        "\n",
        "# Add patterns to the matcher\n",
        "for pattern in skills_patterns:\n",
        "    matcher.add(pattern['label'], [pattern['pattern']])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1452286-d963-4070-8e52-3999c5a90247",
      "metadata": {
        "id": "e1452286-d963-4070-8e52-3999c5a90247"
      },
      "source": [
        "# PREDICT SKILLS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "112a25ad-3080-4ed6-a35e-5852ad372c40",
      "metadata": {
        "id": "112a25ad-3080-4ed6-a35e-5852ad372c40",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Example of a new job description\n",
        "new_job_desc = \"We are looking for a data scientist with experience in machine learning and Python and SQL. Knowledge in database and version control and webscrapping.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "f5189c9f-e20f-4071-99be-6d5bc85a6d28",
      "metadata": {
        "id": "f5189c9f-e20f-4071-99be-6d5bc85a6d28",
        "tags": []
      },
      "outputs": [],
      "source": [
        "text_preprocessor = TextPreprocessor(processing_mode='none')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "b473aec9-e97c-4156-94ec-4354da850543",
      "metadata": {
        "id": "b473aec9-e97c-4156-94ec-4354da850543",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Preprocess the job description text\n",
        "new_job_cleaned = text_preprocessor.preprocess(new_job_desc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "ed860140-73e9-42b7-89c6-fcdb55ae0a49",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "ed860140-73e9-42b7-89c6-fcdb55ae0a49",
        "outputId": "dc722657-1501-4031-b4ce-ebd76abe4437",
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'we are looking for a data scientist with experience in machine learning and python and sql knowledge in database and version control and webscrapping'"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "new_job_cleaned"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "ef5996a0-304c-460f-aa81-0d1f9facd8b3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ef5996a0-304c-460f-aa81-0d1f9facd8b3",
        "outputId": "de49032b-6ee3-4eef-b197-6ed931a41904",
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['database', 'machine learning', 'python']"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "skills_matched = list(find_skills(new_job_cleaned))\n",
        "skills_matched"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "b590745c-8030-4751-848a-fe30cccabb03",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b590745c-8030-4751-848a-fe30cccabb03",
        "outputId": "1053f6fd-248e-440f-b1d6-ec79161bc5d2",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Load the model and tokenizer only once\n",
        "model_name = \"bert-base-uncased\"\n",
        "# Load the model and tokenizer only once\n",
        "model, tokenizer, device = load_model(model_name)\n",
        "job_desc_embedding = get_embeddings(new_job_cleaned, model_name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "e25c9aad-fc43-454b-8f90-cb23b660b22a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e25c9aad-fc43-454b-8f90-cb23b660b22a",
        "outputId": "f0d4b376-1aa9-48f1-84d7-038651d6166b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('agile-project-management', 0.6624161),\n",
              " ('amazon-elasticsearch-service', 0.6521224),\n",
              " ('amazon-machine-learning', 0.6719476),\n",
              " ('amazon-rds-for-postgresql', 0.69189787),\n",
              " ('azure-machine-learning', 0.66870147),\n",
              " ('beta-by-crashlytics', 0.6519042),\n",
              " ('big-data-tools', 0.6576272),\n",
              " ('boosting-machine-learning', 0.6513903),\n",
              " ('computer-multitasking', 0.6706059),\n",
              " ('document-layout-analysis', 0.6584076),\n",
              " ('documentation-as-a-service--tools', 0.6722325),\n",
              " ('exploratory-data-analysis', 0.66050017),\n",
              " ('frameworks-full-stack', 0.6566205),\n",
              " ('google-cloud-dataflow', 0.6509979),\n",
              " ('humancomputer-information-retrieval', 0.7199203),\n",
              " ('humancomputer-interaction', 0.67685163),\n",
              " ('java-build-tools', 0.6551467),\n",
              " ('js-build-tools--js-task-runners', 0.6532289),\n",
              " ('load-and-performance-testing', 0.65299636),\n",
              " ('mobile-prototyping--interaction-design-tools', 0.6913935),\n",
              " ('mobile-testing-frameworks', 0.66996217),\n",
              " ('node.js-process-manager', 0.65105206),\n",
              " ('nosql-database-as-a-service', 0.6969453),\n",
              " ('object-document-mapper-odm', 0.6695248),\n",
              " ('object-relational-mapper-orm', 0.65396535),\n",
              " ('open-postgresql-monitoring', 0.6734212),\n",
              " ('postgresql-as-a-service', 0.653222),\n",
              " ('self-hosted-blogging--cms', 0.6654465),\n",
              " ('server-configuration-and-automation', 0.656839),\n",
              " ('spreadsheets-as-a-backend', 0.6859241),\n",
              " ('sqlalchemy', 0.67683417),\n",
              " ('testing-frameworks', 0.65124714),\n",
              " ('tools-for-text-editors', 0.6552689),\n",
              " ('web-mining', 0.6540476)]"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "THRESHOLD = 0.65\n",
        "top_skills = find_top_skills(job_desc_embedding, skill_embeddings, skills_list, THRESHOLD)\n",
        "top_skills"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "zrWnY4yYZBSq",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zrWnY4yYZBSq",
        "outputId": "9748e6e0-1ae0-4145-d03f-9851a4343751"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['agile project management',\n",
              " 'amazon elasticsearch service',\n",
              " 'amazon machine learning',\n",
              " 'amazon rds for postgresql',\n",
              " 'azure machine learning',\n",
              " 'beta by crashlytics',\n",
              " 'big data tools',\n",
              " 'boosting machine learning',\n",
              " 'computer multitasking',\n",
              " 'document layout analysis',\n",
              " 'documentation as a service  tools',\n",
              " 'exploratory data analysis',\n",
              " 'frameworks full stack',\n",
              " 'google cloud dataflow',\n",
              " 'humancomputer information retrieval',\n",
              " 'humancomputer interaction',\n",
              " 'java build tools',\n",
              " 'js build tools  js task runners',\n",
              " 'load and performance testing',\n",
              " 'mobile prototyping  interaction design tools',\n",
              " 'mobile testing frameworks',\n",
              " 'node.js process manager',\n",
              " 'nosql database as a service',\n",
              " 'object document mapper odm',\n",
              " 'object relational mapper orm',\n",
              " 'open postgresql monitoring',\n",
              " 'postgresql as a service',\n",
              " 'self hosted blogging  cms',\n",
              " 'server configuration and automation',\n",
              " 'spreadsheets as a backend',\n",
              " 'sqlalchemy',\n",
              " 'testing frameworks',\n",
              " 'tools for text editors',\n",
              " 'web mining']"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "skills_list, scores_list = zip(*top_skills)\n",
        "#format the skills without the -\n",
        "skills_list = [skill.replace('-', ' ') for skill in skills_list]\n",
        "skills_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "h9ADMpsCb2DV",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h9ADMpsCb2DV",
        "outputId": "307f5227-4976-45df-b2b9-f10368d4fd1f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['load and performance testing',\n",
              " 'spreadsheets as a backend',\n",
              " 'frameworks full stack',\n",
              " 'nosql database as a service',\n",
              " 'computer multitasking',\n",
              " 'document layout analysis',\n",
              " 'node.js process manager',\n",
              " 'tools for text editors',\n",
              " 'amazon rds for postgresql',\n",
              " 'big data tools',\n",
              " 'mobile prototyping  interaction design tools',\n",
              " 'humancomputer information retrieval',\n",
              " 'js build tools  js task runners',\n",
              " 'database',\n",
              " 'azure machine learning',\n",
              " 'amazon machine learning',\n",
              " 'self hosted blogging  cms',\n",
              " 'server configuration and automation',\n",
              " 'machine learning',\n",
              " 'boosting machine learning',\n",
              " 'agile project management',\n",
              " 'exploratory data analysis',\n",
              " 'amazon elasticsearch service',\n",
              " 'mobile testing frameworks',\n",
              " 'object relational mapper orm',\n",
              " 'testing frameworks',\n",
              " 'humancomputer interaction',\n",
              " 'web mining',\n",
              " 'postgresql as a service',\n",
              " 'beta by crashlytics',\n",
              " 'documentation as a service  tools',\n",
              " 'java build tools',\n",
              " 'open postgresql monitoring',\n",
              " 'google cloud dataflow',\n",
              " 'object document mapper odm',\n",
              " 'sqlalchemy',\n",
              " 'python']"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "total_list = list(set(skills_list + skills_matched))\n",
        "total_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "I9LDpo22b6wu",
      "metadata": {
        "id": "I9LDpo22b6wu"
      },
      "outputs": [],
      "source": [
        "#pip install streamlit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f48df9a",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
