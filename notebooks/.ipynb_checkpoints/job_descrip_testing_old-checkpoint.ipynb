{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#File manipulation\n",
    "import os\n",
    "\n",
    "#Text processing and cleaning\n",
    "import contractions # To include english contractions\n",
    "import re #regex\n",
    "import string #used to include punctuation during text processing\n",
    "from collections import Counter #count strings in texts\n",
    "\n",
    "#Natural Language Tool Kit NLK package\n",
    "import nltk\n",
    "from nltk.corpus import stopwords #Stopwords\n",
    "from nltk.tokenize  import sent_tokenize ,  word_tokenize # Word and sentence tokenizer\n",
    "from nltk import pos_tag, ngrams #N-grams analysis\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer #Lemmatizer and Stemmer\n",
    "from nltk.text import Text #for concordance\n",
    "from nltk.collocations import * \n",
    "from nltk.collocations import BigramCollocationFinder, TrigramCollocationFinder #collocations\n",
    "from nltk import BigramAssocMeasures, TrigramAssocMeasures  # Measures for evaluating bigram associations\n",
    "from nltk import bigrams # Generate bigrams from text data\n",
    "\n",
    "\n",
    "#Data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "import networkx as nx #Used for network graph\n",
    "\n",
    "#Topic modeling/ Clustering\n",
    "from sklearn.feature_extraction.text import CountVectorizer # Bag-of-Words model\n",
    "from sklearn.decomposition import LatentDirichletAllocation #LDA for topic modeling\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer #term-frequency inverse document frequency vectorizer\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage #Hierarchical clustering\n",
    "from sklearn.metrics import homogeneity_score, completeness_score #for quality of cluestering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/rodzaraya/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/rodzaraya/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/rodzaraya/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company_name</th>\n",
       "      <th>job_description</th>\n",
       "      <th>position_title</th>\n",
       "      <th>description_length</th>\n",
       "      <th>model_response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Google</td>\n",
       "      <td>minimum qualifications\\nbachelors degree or eq...</td>\n",
       "      <td>Sales Specialist</td>\n",
       "      <td>2727</td>\n",
       "      <td>{\\n  \"Core Responsibilities\": \"Responsible fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Apple</td>\n",
       "      <td>description\\nas an asc you will be highly infl...</td>\n",
       "      <td>Apple Solutions Consultant</td>\n",
       "      <td>828</td>\n",
       "      <td>{\\n  \"Core Responsibilities\": \"as an asc you ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Netflix</td>\n",
       "      <td>its an amazing time to be joining netflix as w...</td>\n",
       "      <td>Licensing Coordinator - Consumer Products</td>\n",
       "      <td>3205</td>\n",
       "      <td>{\\n  \"Core Responsibilities\": \"Help drive bus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Robert Half</td>\n",
       "      <td>description\\n\\nweb designers looking to expand...</td>\n",
       "      <td>Web Designer</td>\n",
       "      <td>2489</td>\n",
       "      <td>{\\n  \"Core Responsibilities\": \"Designing webs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TrackFive</td>\n",
       "      <td>at trackfive weve got big goals were on a miss...</td>\n",
       "      <td>Web Developer</td>\n",
       "      <td>3167</td>\n",
       "      <td>{\\n  \"Core Responsibilities\": \"Build and layo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>848</th>\n",
       "      <td>Menards</td>\n",
       "      <td>job description\\n\\nparttime\\n\\nmake big money ...</td>\n",
       "      <td>Management Internship</td>\n",
       "      <td>1122</td>\n",
       "      <td>{\\n  \"Core Responsibilities\": \"Responsibiliti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>849</th>\n",
       "      <td>Parker</td>\n",
       "      <td>responsibilities\\nparkers internship program w...</td>\n",
       "      <td>Human Resources Internship - Corporate  (Year-...</td>\n",
       "      <td>3840</td>\n",
       "      <td>{\\n  \"Core Responsibilities\": \"Assist in gene...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>850</th>\n",
       "      <td>Borgen Project</td>\n",
       "      <td>the borgen project is an innovative national ...</td>\n",
       "      <td>Writer / Journalist Internship</td>\n",
       "      <td>897</td>\n",
       "      <td>{\\n  \"Core Responsibilities\": \"Write one arti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851</th>\n",
       "      <td>Wyndham Destinations</td>\n",
       "      <td>put the world on vacation\\n\\nat wyndham destin...</td>\n",
       "      <td>Inbound Customer Service / Sales (Remote)</td>\n",
       "      <td>4604</td>\n",
       "      <td>{\\n  \"Core Responsibilities\": \"Answer inbound...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>852</th>\n",
       "      <td>Aerotek</td>\n",
       "      <td>this job handles customer inquiries by telepho...</td>\n",
       "      <td>Remote Inbound Customer Service Representative</td>\n",
       "      <td>1592</td>\n",
       "      <td>{\\n  \"Core Responsibilities\": \"Handle incomin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>853 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             company_name                                    job_description   \n",
       "0                  Google  minimum qualifications\\nbachelors degree or eq...  \\\n",
       "1                   Apple  description\\nas an asc you will be highly infl...   \n",
       "2                 Netflix  its an amazing time to be joining netflix as w...   \n",
       "3             Robert Half  description\\n\\nweb designers looking to expand...   \n",
       "4               TrackFive  at trackfive weve got big goals were on a miss...   \n",
       "..                    ...                                                ...   \n",
       "848               Menards  job description\\n\\nparttime\\n\\nmake big money ...   \n",
       "849                Parker  responsibilities\\nparkers internship program w...   \n",
       "850        Borgen Project   the borgen project is an innovative national ...   \n",
       "851  Wyndham Destinations  put the world on vacation\\n\\nat wyndham destin...   \n",
       "852               Aerotek  this job handles customer inquiries by telepho...   \n",
       "\n",
       "                                        position_title  description_length   \n",
       "0                                     Sales Specialist                2727  \\\n",
       "1                           Apple Solutions Consultant                 828   \n",
       "2            Licensing Coordinator - Consumer Products                3205   \n",
       "3                                         Web Designer                2489   \n",
       "4                                        Web Developer                3167   \n",
       "..                                                 ...                 ...   \n",
       "848                              Management Internship                1122   \n",
       "849  Human Resources Internship - Corporate  (Year-...                3840   \n",
       "850                     Writer / Journalist Internship                 897   \n",
       "851          Inbound Customer Service / Sales (Remote)                4604   \n",
       "852     Remote Inbound Customer Service Representative                1592   \n",
       "\n",
       "                                        model_response  \n",
       "0     {\\n  \"Core Responsibilities\": \"Responsible fo...  \n",
       "1     {\\n  \"Core Responsibilities\": \"as an asc you ...  \n",
       "2     {\\n  \"Core Responsibilities\": \"Help drive bus...  \n",
       "3     {\\n  \"Core Responsibilities\": \"Designing webs...  \n",
       "4     {\\n  \"Core Responsibilities\": \"Build and layo...  \n",
       "..                                                 ...  \n",
       "848   {\\n  \"Core Responsibilities\": \"Responsibiliti...  \n",
       "849   {\\n  \"Core Responsibilities\": \"Assist in gene...  \n",
       "850   {\\n  \"Core Responsibilities\": \"Write one arti...  \n",
       "851   {\\n  \"Core Responsibilities\": \"Answer inbound...  \n",
       "852   {\\n  \"Core Responsibilities\": \"Handle incomin...  \n",
       "\n",
       "[853 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/job_descriptions/training_Data.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' {\\n  \"Core Responsibilities\": \"Responsible for expanding Google Workspace product adoption across an assigned territory. Build relationships with customers to understand needs and provide Google Workspace solutions. Partner with account teams to construct solutions and grow business for Google Workspace.\",\\n  \"Required Skills\": \"Bachelor\\'s degree or equivalent experience. Experience managing enterprise SaaS accounts and sales cycles.\", \\n  \"Educational Requirements\": \"Bachelor\\'s degree or equivalent experience.\",\\n  \"Experience Level\": \"Experience managing enterprise SaaS accounts and sales cycles.\",\\n  \"Preferred Qualifications\": \"Experience building strategic partnerships with enterprise customers. Ability to work through a reseller ecosystem. Excellent communication and strategic thinking skills.\",\\n  \"Compensation and Benefits\": \"N/A\"\\n}'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.model_response[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPreprocessor:\n",
    "    def __init__(self, processing_mode='none', custom_punctuation=None, custom_stopwords=None, sentence_analysis=False):\n",
    "        \"\"\"\n",
    "            Initialization considers Custom punctuation, Stop words, and Lemmatizer or Stemmer.\n",
    "            Updates custom punctuation and custom stop words set with additional ones if provided.\n",
    "            The processing mode to standardise variants can be choose between none, Stem and Lemma. Each mode is stored in a different\n",
    "            column of the dataframe.\n",
    "            Sentence analysis parameter is used to keep the punctuation symbols required for sentence analysis.\n",
    "\n",
    "            Parameters:\n",
    "            - processing_mode: String to decide whether to use 'lemma', 'stem', or 'none' for text processing.\n",
    "            - custom_punctuation: Additional punctuation characters to remove from text.\n",
    "            - custom_stopwords: Additional stopwords to remove from text.\n",
    "            - sentence_analysis: Boolean indicating sentence analysis cleaning steps. This mode will keep the punctuation symbols. \n",
    "            \n",
    "            \"\"\"\n",
    "        \n",
    "        self.punctuation = string.punctuation #Init with all punctuation characters\n",
    "        \n",
    "        if custom_punctuation:\n",
    "            self.punctuation += custom_punctuation #add custom punctuation\n",
    "\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        if custom_stopwords:\n",
    "            self.stop_words.update(custom_stopwords) #add custom stopwords\n",
    "        \n",
    "        # Determine which text processing mode to use\n",
    "        self.processing_mode = processing_mode.lower()\n",
    "        \n",
    "        # Set the sentence analysis mode\n",
    "        self.sentence_analysis = sentence_analysis\n",
    "        \n",
    "        #Set the variant standardization mode\n",
    "        if self.processing_mode == 'lemma':\n",
    "            self.lemmatizer = WordNetLemmatizer()\n",
    "        elif self.processing_mode == 'stem':\n",
    "            self.stemmer = PorterStemmer()\n",
    "\n",
    "    # Expand contractions using the contractions library\n",
    "    def expand_contractions(self, text):\n",
    "        return contractions.fix(text)\n",
    "\n",
    "    # Split hyphenated words into separate words, like phone numbers or radio fm, age, etc.\n",
    "    def split_hyphenated_words(self, text):\n",
    "        return re.sub(r'-', ' ', text)\n",
    "\n",
    "    def remove_punctuation(self, text):\n",
    "        return ''.join([char for char in text if char not in self.punctuation])\n",
    "\n",
    "    def add_space_after_parenthesis(self, text):\n",
    "        return re.sub(r'\\)', ') ', text)\n",
    "\n",
    "    def to_lowercase(self, text):\n",
    "        return text.lower()\n",
    "\n",
    "    def remove_stopwords(self, text):\n",
    "        words = word_tokenize(text)\n",
    "        return ' '.join([word for word in words if word not in self.stop_words])\n",
    "\n",
    "    def remove_extra_whitespace(self, text):\n",
    "        return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    def stem_words(self, text):\n",
    "        words = word_tokenize(text)\n",
    "        return ' '.join([self.stemmer.stem(word) for word in words])\n",
    "\n",
    "    def lemmatize_words(self, text):\n",
    "        words = word_tokenize(text)\n",
    "        return ' '.join([self.lemmatizer.lemmatize(word) for word in words])\n",
    "\n",
    "    # Order matters\n",
    "    def preprocess(self, text):\n",
    "        text = self.expand_contractions(text)\n",
    "        text = self.split_hyphenated_words(text)\n",
    "        text = self.add_space_after_parenthesis(text)\n",
    "        \n",
    "        #In case we need to analyse sentences, we will need the punctuations\n",
    "        if not self.sentence_analysis:\n",
    "            text = self.remove_punctuation(text)\n",
    "        text = self.to_lowercase(text)\n",
    "        #The stopwords are removed if the users wants to standardise variants.\n",
    "        #If none is selected, the ouput will just perform previous cleaning steps\n",
    "        if self.processing_mode != 'none':\n",
    "            text = self.remove_stopwords(text)\n",
    "            \n",
    "        text = self.remove_extra_whitespace(text)\n",
    "        \n",
    "        #Select the processing mode for variants\n",
    "        if self.processing_mode == 'lemma':\n",
    "            text = self.lemmatize_words(text)\n",
    "        elif self.processing_mode == 'stem':\n",
    "            text = self.stem_words(text)\n",
    "        \n",
    "        return text\n",
    "\n",
    "    #Apply preprocessing steps to daframe and create a column base on the processing mode\n",
    "    def preprocess_dataframe(self, df, column_name):\n",
    "        if not self.sentence_analysis:\n",
    "            if self.processing_mode == 'lemma':\n",
    "                df['processed_lemma'] = df[column_name].apply(self.preprocess)\n",
    "            elif self.processing_mode == 'stem':\n",
    "                df['processed_stem'] = df[column_name].apply(self.preprocess)\n",
    "            else:  # If 'none', apply preprocessing without lemma or stem\n",
    "                df['processed_cleaned'] = df[column_name].apply(self.preprocess)\n",
    "        else: # Add different processed columns for sentences\n",
    "            if self.processing_mode == 'lemma':\n",
    "                df['processed_lemma_sent'] = df[column_name].apply(self.preprocess)\n",
    "            elif self.processing_mode == 'stem':\n",
    "                df['processed_stem_sent'] = df[column_name].apply(self.preprocess)\n",
    "            else:  # If 'none', apply preprocessing without lemma or stem\n",
    "                df['processed_cleaned_sent'] = df[column_name].apply(self.preprocess)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company_name</th>\n",
       "      <th>job_description</th>\n",
       "      <th>position_title</th>\n",
       "      <th>description_length</th>\n",
       "      <th>model_response</th>\n",
       "      <th>processed_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Google</td>\n",
       "      <td>minimum qualifications\\nbachelors degree or eq...</td>\n",
       "      <td>Sales Specialist</td>\n",
       "      <td>2727</td>\n",
       "      <td>{\\n  \"Core Responsibilities\": \"Responsible fo...</td>\n",
       "      <td>minimum qualifications bachelors degree or equ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Apple</td>\n",
       "      <td>description\\nas an asc you will be highly infl...</td>\n",
       "      <td>Apple Solutions Consultant</td>\n",
       "      <td>828</td>\n",
       "      <td>{\\n  \"Core Responsibilities\": \"as an asc you ...</td>\n",
       "      <td>description as an asc you will be highly influ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Netflix</td>\n",
       "      <td>its an amazing time to be joining netflix as w...</td>\n",
       "      <td>Licensing Coordinator - Consumer Products</td>\n",
       "      <td>3205</td>\n",
       "      <td>{\\n  \"Core Responsibilities\": \"Help drive bus...</td>\n",
       "      <td>its an amazing time to be joining netflix as w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Robert Half</td>\n",
       "      <td>description\\n\\nweb designers looking to expand...</td>\n",
       "      <td>Web Designer</td>\n",
       "      <td>2489</td>\n",
       "      <td>{\\n  \"Core Responsibilities\": \"Designing webs...</td>\n",
       "      <td>description web designers looking to expand yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TrackFive</td>\n",
       "      <td>at trackfive weve got big goals were on a miss...</td>\n",
       "      <td>Web Developer</td>\n",
       "      <td>3167</td>\n",
       "      <td>{\\n  \"Core Responsibilities\": \"Build and layo...</td>\n",
       "      <td>at trackfive we have got big goals were on a m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>848</th>\n",
       "      <td>Menards</td>\n",
       "      <td>job description\\n\\nparttime\\n\\nmake big money ...</td>\n",
       "      <td>Management Internship</td>\n",
       "      <td>1122</td>\n",
       "      <td>{\\n  \"Core Responsibilities\": \"Responsibiliti...</td>\n",
       "      <td>job description parttime make big money at men...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>849</th>\n",
       "      <td>Parker</td>\n",
       "      <td>responsibilities\\nparkers internship program w...</td>\n",
       "      <td>Human Resources Internship - Corporate  (Year-...</td>\n",
       "      <td>3840</td>\n",
       "      <td>{\\n  \"Core Responsibilities\": \"Assist in gene...</td>\n",
       "      <td>responsibilities parkers internship program wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>850</th>\n",
       "      <td>Borgen Project</td>\n",
       "      <td>the borgen project is an innovative national ...</td>\n",
       "      <td>Writer / Journalist Internship</td>\n",
       "      <td>897</td>\n",
       "      <td>{\\n  \"Core Responsibilities\": \"Write one arti...</td>\n",
       "      <td>the borgen project is an innovative national c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851</th>\n",
       "      <td>Wyndham Destinations</td>\n",
       "      <td>put the world on vacation\\n\\nat wyndham destin...</td>\n",
       "      <td>Inbound Customer Service / Sales (Remote)</td>\n",
       "      <td>4604</td>\n",
       "      <td>{\\n  \"Core Responsibilities\": \"Answer inbound...</td>\n",
       "      <td>put the world on vacation at wyndham destinati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>852</th>\n",
       "      <td>Aerotek</td>\n",
       "      <td>this job handles customer inquiries by telepho...</td>\n",
       "      <td>Remote Inbound Customer Service Representative</td>\n",
       "      <td>1592</td>\n",
       "      <td>{\\n  \"Core Responsibilities\": \"Handle incomin...</td>\n",
       "      <td>this job handles customer inquiries by telepho...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>853 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             company_name                                    job_description   \n",
       "0                  Google  minimum qualifications\\nbachelors degree or eq...  \\\n",
       "1                   Apple  description\\nas an asc you will be highly infl...   \n",
       "2                 Netflix  its an amazing time to be joining netflix as w...   \n",
       "3             Robert Half  description\\n\\nweb designers looking to expand...   \n",
       "4               TrackFive  at trackfive weve got big goals were on a miss...   \n",
       "..                    ...                                                ...   \n",
       "848               Menards  job description\\n\\nparttime\\n\\nmake big money ...   \n",
       "849                Parker  responsibilities\\nparkers internship program w...   \n",
       "850        Borgen Project   the borgen project is an innovative national ...   \n",
       "851  Wyndham Destinations  put the world on vacation\\n\\nat wyndham destin...   \n",
       "852               Aerotek  this job handles customer inquiries by telepho...   \n",
       "\n",
       "                                        position_title  description_length   \n",
       "0                                     Sales Specialist                2727  \\\n",
       "1                           Apple Solutions Consultant                 828   \n",
       "2            Licensing Coordinator - Consumer Products                3205   \n",
       "3                                         Web Designer                2489   \n",
       "4                                        Web Developer                3167   \n",
       "..                                                 ...                 ...   \n",
       "848                              Management Internship                1122   \n",
       "849  Human Resources Internship - Corporate  (Year-...                3840   \n",
       "850                     Writer / Journalist Internship                 897   \n",
       "851          Inbound Customer Service / Sales (Remote)                4604   \n",
       "852     Remote Inbound Customer Service Representative                1592   \n",
       "\n",
       "                                        model_response   \n",
       "0     {\\n  \"Core Responsibilities\": \"Responsible fo...  \\\n",
       "1     {\\n  \"Core Responsibilities\": \"as an asc you ...   \n",
       "2     {\\n  \"Core Responsibilities\": \"Help drive bus...   \n",
       "3     {\\n  \"Core Responsibilities\": \"Designing webs...   \n",
       "4     {\\n  \"Core Responsibilities\": \"Build and layo...   \n",
       "..                                                 ...   \n",
       "848   {\\n  \"Core Responsibilities\": \"Responsibiliti...   \n",
       "849   {\\n  \"Core Responsibilities\": \"Assist in gene...   \n",
       "850   {\\n  \"Core Responsibilities\": \"Write one arti...   \n",
       "851   {\\n  \"Core Responsibilities\": \"Answer inbound...   \n",
       "852   {\\n  \"Core Responsibilities\": \"Handle incomin...   \n",
       "\n",
       "                                     processed_cleaned  \n",
       "0    minimum qualifications bachelors degree or equ...  \n",
       "1    description as an asc you will be highly influ...  \n",
       "2    its an amazing time to be joining netflix as w...  \n",
       "3    description web designers looking to expand yo...  \n",
       "4    at trackfive we have got big goals were on a m...  \n",
       "..                                                 ...  \n",
       "848  job description parttime make big money at men...  \n",
       "849  responsibilities parkers internship program wa...  \n",
       "850  the borgen project is an innovative national c...  \n",
       "851  put the world on vacation at wyndham destinati...  \n",
       "852  this job handles customer inquiries by telepho...  \n",
       "\n",
       "[853 rows x 6 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_preprocessor = TextPreprocessor(processing_mode='none')\n",
    "df = text_preprocessor.preprocess_dataframe(df, 'job_description')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minimum qualifications bachelors degree or equivalent practical experience years of experience in saas or productivity tools businessexperience managing enterprise accounts with sales cycles preferred qualifications years of experience building strategic business partnerships with enterprise customersability to work through and with a reseller ecosystem to scale the businessability to plan pitch and execute a territory business strategyability to build relationships and to deliver results in a crossfunctionalmatrixed environmentability to identify crosspromoting and uppromoting opportunities within the existing account baseexcellent account management writtenverbal communication strategic and analyticalthinking skills about the job as a member of the google cloud team you inspire leading companies schools and government agencies to work smarter with google tools like google workspace search and chrome you advocate the innovative power of our products to make organizations more productive collaborative and mobile your guiding light is doing what is right for the customer you will meet customers exactly where they are at and provide them the best solutions for innovation using your passion for google products you help spread the magic of google to organizations around the world the google workspace team helps customers transform and evolve their business through the use of googles productivity collaboration and content management suite of applications as part of an entrepreneurial team in this growing business you will help shape the future of businesses use technology to connect with customers employees and partners as a google workspace sales specialist you will be responsible for maintenance and expansion of google workspace business growth across the region with customers in this role you will create and execute the strategy and provide unique insights on applying google workspace solutions to enterprisesyou will build an excellent pipeline and work with the account teams to build out the customer solution and establish partnerships you will strategize with partners to increase account and territory business growth you will work directly with customers coordinate internal resources and construct successful strategies at account and territory level google cloud accelerates organizations ability to digitally transform their business with the best infrastructure platform industry solutions and expertise we deliver enterprisegrade solutions that leverage googles cuttingedge technology all on the cleanest cloud in the industry customers in more than countries and territories turn to google cloud as their trusted partner to enable growth and solve their most critical business problems\n"
     ]
    }
   ],
   "source": [
    "print(df.processed_cleaned[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub('[^a-zA-Z]', ' ', text)\n",
    "    sentences = sent_tokenize(text)\n",
    "    features = {'feature': \"\"}\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    for sent in sentences:\n",
    "        if any(criteria in sent for criteria in ['skills', 'education']):\n",
    "            words = word_tokenize(sent)\n",
    "            words = [word for word in words if word not in stop_words]\n",
    "            tagged_words = pos_tag(words) # Part of speech\n",
    "            filtered_words = [word for word, tag in tagged_words if tag not in ['DT', 'IN', 'TO', 'PRP', 'WP']]\n",
    "            features['feature'] += \" \".join(filtered_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      minimum qualifications bachelors degree equiva...\n",
       "1      description asc highly influential growing min...\n",
       "2      amazing time joining netflix continue transfor...\n",
       "3      description web designers looking expand profe...\n",
       "4      trackfive weve got big goals mission revolutio...\n",
       "                             ...                        \n",
       "848    job description parttime make big money menard...\n",
       "849    responsibilities parkers internship program es...\n",
       "850    borgen project innovative national campaign wo...\n",
       "851                                                     \n",
       "852    job handles customer inquiries telephone andor...\n",
       "Name: Features, Length: 853, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Features'] = df['job_description'].apply(lambda x : preprocess_text(x)['feature'])\n",
    "df['Features']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "def get_embeddings(text, model_name):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    inputs = tokenizer(str(text), return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "    outputs = model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1).detach().to(\"cpu\").numpy()\n",
    "    return embeddings\n",
    "\n",
    "# Select device (MPS for Mac, CUDA for NVIDIA GPUs, CPU as a fallback)\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")   \n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "model.to(device)\n",
    "\n",
    "job_desc_embeddings = np.array([get_embeddings(desc, model_name) for desc in df['Features']]).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rodzaraya/opt/anaconda3/lib/python3.9/site-packages/spacy/util.py:275: UserWarning: [W031] Model 'en_core_web_sm' (2.2.0) requires spaCy v2.2 and is incompatible with the current spaCy version (2.3.5). This may lead to unexpected results or runtime errors. To resolve this, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "import json\n",
    "\n",
    "# Load the spaCy model (small English model is sufficient for matching)\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Features</th>\n",
       "      <th>Skills Found</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>minimum qualifications bachelors degree equiva...</td>\n",
       "      <td>{chrome, business, collaboration, content mana...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>description asc highly influential growing min...</td>\n",
       "      <td>{business}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>amazing time joining netflix continue transfor...</td>\n",
       "      <td>{languages, business, play, swift, schedule, w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>description web designers looking expand profe...</td>\n",
       "      <td>{adobe photoshop, javascript, testing, account...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>trackfive weve got big goals mission revolutio...</td>\n",
       "      <td>{security, javascript, software, databases, co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>848</th>\n",
       "      <td>job description parttime make big money menard...</td>\n",
       "      <td>{business}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>849</th>\n",
       "      <td>responsibilities parkers internship program es...</td>\n",
       "      <td>{testing, business, project management, softwa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>850</th>\n",
       "      <td>borgen project innovative national campaign wo...</td>\n",
       "      <td>{schedule}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851</th>\n",
       "      <td></td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>852</th>\n",
       "      <td>job handles customer inquiries telephone andor...</td>\n",
       "      <td>{support}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>853 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Features   \n",
       "0    minimum qualifications bachelors degree equiva...  \\\n",
       "1    description asc highly influential growing min...   \n",
       "2    amazing time joining netflix continue transfor...   \n",
       "3    description web designers looking expand profe...   \n",
       "4    trackfive weve got big goals mission revolutio...   \n",
       "..                                                 ...   \n",
       "848  job description parttime make big money menard...   \n",
       "849  responsibilities parkers internship program es...   \n",
       "850  borgen project innovative national campaign wo...   \n",
       "851                                                      \n",
       "852  job handles customer inquiries telephone andor...   \n",
       "\n",
       "                                          Skills Found  \n",
       "0    {chrome, business, collaboration, content mana...  \n",
       "1                                           {business}  \n",
       "2    {languages, business, play, swift, schedule, w...  \n",
       "3    {adobe photoshop, javascript, testing, account...  \n",
       "4    {security, javascript, software, databases, co...  \n",
       "..                                                 ...  \n",
       "848                                         {business}  \n",
       "849  {testing, business, project management, softwa...  \n",
       "850                                         {schedule}  \n",
       "851                                                 {}  \n",
       "852                                          {support}  \n",
       "\n",
       "[853 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Initialize the Matcher with the shared vocabulary\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Load patterns from the JSONL file\n",
    "patterns = []\n",
    "with open('../data/jz_skill_patterns.jsonl', 'r') as f:\n",
    "    for line in f:\n",
    "        patterns.append(json.loads(line))\n",
    "\n",
    "# Add patterns to the matcher\n",
    "for pattern in patterns:\n",
    "    matcher.add(pattern['label'], [pattern['pattern']])\n",
    "\n",
    "# Define a function to apply the matcher and find skills in the text\n",
    "def find_skills(text):\n",
    "    doc = nlp(text)\n",
    "    matches = matcher(doc)\n",
    "    skills = set()  # To store found skills\n",
    "    for match_id, start, end in matches:\n",
    "        skill = doc[start:end].text\n",
    "        skills.add(skill)\n",
    "    return skills\n",
    "\n",
    "# Apply the find_skills function to the 'Features' column of the DataFrame\n",
    "df['Skills Found'] = df['Features'].apply(find_skills)\n",
    "\n",
    "# Display the new column with the found skills\n",
    "df[['Features', 'Skills Found']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       {\\n  \"Core Responsibilities\": \"Responsible fo...\n",
       "1       {\\n  \"Core Responsibilities\": \"as an asc you ...\n",
       "2       {\\n  \"Core Responsibilities\": \"Help drive bus...\n",
       "3       {\\n  \"Core Responsibilities\": \"Designing webs...\n",
       "4       {\\n  \"Core Responsibilities\": \"Build and layo...\n",
       "                             ...                        \n",
       "848     {\\n  \"Core Responsibilities\": \"Responsibiliti...\n",
       "849     {\\n  \"Core Responsibilities\": \"Assist in gene...\n",
       "850     {\\n  \"Core Responsibilities\": \"Write one arti...\n",
       "851     {\\n  \"Core Responsibilities\": \"Answer inbound...\n",
       "852     {\\n  \"Core Responsibilities\": \"Handle incomin...\n",
       "Name: model_response, Length: 853, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.model_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_response</th>\n",
       "      <th>Extracted_Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{\\n  \"Core Responsibilities\": \"Responsible fo...</td>\n",
       "      <td>Bachelor's degree or equivalent experience. Ex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{\\n  \"Core Responsibilities\": \"as an asc you ...</td>\n",
       "      <td>a passion to help people understand how apple ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{\\n  \"Core Responsibilities\": \"Help drive bus...</td>\n",
       "      <td>2+ years experience in preferably outbound lic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{\\n  \"Core Responsibilities\": \"Designing webs...</td>\n",
       "      <td>2+ years experience in web design. Proficiency...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{\\n  \"Core Responsibilities\": \"Build and layo...</td>\n",
       "      <td>2+ years of experience with HTML and CSS/SASS,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      model_response   \n",
       "0   {\\n  \"Core Responsibilities\": \"Responsible fo...  \\\n",
       "1   {\\n  \"Core Responsibilities\": \"as an asc you ...   \n",
       "2   {\\n  \"Core Responsibilities\": \"Help drive bus...   \n",
       "3   {\\n  \"Core Responsibilities\": \"Designing webs...   \n",
       "4   {\\n  \"Core Responsibilities\": \"Build and layo...   \n",
       "\n",
       "                                      Extracted_Text  \n",
       "0  Bachelor's degree or equivalent experience. Ex...  \n",
       "1  a passion to help people understand how apple ...  \n",
       "2  2+ years experience in preferably outbound lic...  \n",
       "3  2+ years experience in web design. Proficiency...  \n",
       "4  2+ years of experience with HTML and CSS/SASS,...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_skills_from_json(json_data):\n",
    "    # Parse the model_response (JSON format in string form)\n",
    "    parsed = json.loads(json_data)\n",
    "    \n",
    "    # Extract relevant fields: Required Skills, Educational Requirements, etc.\n",
    "    required_skills = parsed.get(\"Required Skills\", \"\")\n",
    "    education_requirements = parsed.get(\"Educational Requirements\", \"\")\n",
    "    experience_level = parsed.get(\"Experience Level\", \"\")\n",
    "    \n",
    "      # Convert any list fields to strings (join lists with spaces)\n",
    "    if isinstance(required_skills, list):\n",
    "        required_skills = \" \".join(required_skills)\n",
    "    \n",
    "    if isinstance(education_requirements, list):\n",
    "        education_requirements = \" \".join(education_requirements)\n",
    "    \n",
    "    if isinstance(experience_level, list):\n",
    "        experience_level = \" \".join(experience_level)\n",
    "    \n",
    "    # Combine the relevant sections into one string for training\n",
    "    combined_text = required_skills + \" \" + education_requirements + \" \" + experience_level\n",
    "    \n",
    "    return combined_text.strip()\n",
    "    \n",
    "\n",
    "# Apply the function to the 'model_response' column to extract text\n",
    "df['Extracted_Text'] = df['model_response'].apply(extract_skills_from_json)\n",
    "\n",
    "# Display the extracted text\n",
    "df[['model_response', 'Extracted_Text']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spacy\n",
    "\n",
    "from spacy.pipeline import EntityRuler\n",
    "from spacy.lang.en import English\n",
    "from spacy.tokens import Doc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract skills from text using the custom entity ruler\n",
    "def get_skills(text):\n",
    "    doc = nlp(text)\n",
    "    skills = [ent.text for ent in doc.ents if ent.label_ == \"SKILL\"]\n",
    "    return skills\n",
    "\n",
    "# Ensure unique skills\n",
    "def unique_skills(skill_list):\n",
    "    return list(set(skill_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tagger', 'parser', 'entity_ruler', 'ner']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Extracted_Text</th>\n",
       "      <th>skills</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bachelor's degree or equivalent experience. Ex...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a passion to help people understand how apple ...</td>\n",
       "      <td>[business]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2+ years experience in preferably outbound lic...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2+ years experience in web design. Proficiency...</td>\n",
       "      <td>[javascript, content management, design]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2+ years of experience with HTML and CSS/SASS,...</td>\n",
       "      <td>[sass, security, javascript, mysql, databases,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Extracted_Text   \n",
       "0  Bachelor's degree or equivalent experience. Ex...  \\\n",
       "1  a passion to help people understand how apple ...   \n",
       "2  2+ years experience in preferably outbound lic...   \n",
       "3  2+ years experience in web design. Proficiency...   \n",
       "4  2+ years of experience with HTML and CSS/SASS,...   \n",
       "\n",
       "                                              skills  \n",
       "0                                                 []  \n",
       "1                                         [business]  \n",
       "2                                                 []  \n",
       "3           [javascript, content management, design]  \n",
       "4  [sass, security, javascript, mysql, databases,...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an EntityRuler component explicitly using create_pipe and add it to the pipeline\n",
    "ruler = nlp.create_pipe(\"entity_ruler\")\n",
    "nlp.add_pipe(ruler, before=\"ner\")\n",
    "\n",
    "# Load the patterns from a file (assuming 'patterns' is a valid path to the JSONL file)\n",
    "ruler.from_disk('../data/jz_skill_patterns.jsonl')\n",
    "\n",
    "# Check the pipeline components\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "\n",
    "# Apply the function to the 'Extracted_Text' column\n",
    "df[\"skills\"] = df[\"Extracted_Text\"].str.lower().apply(get_skills)\n",
    "\n",
    "# Ensure the skills list contains unique entries\n",
    "df[\"skills\"] = df[\"skills\"].apply(unique_skills)\n",
    "\n",
    "# Show the result\n",
    "df[['Extracted_Text', 'skills']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "skills          [sass, security, javascript, mysql, databases,...\n",
       "Skills Found    {security, javascript, software, databases, co...\n",
       "Name: 4, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['skills','Skills Found']].iloc[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skills loaded from JSON: ['.net', '1password', '3d', '3d-reconstruction', 'aboutness', 'abstract-data-type', 'abstract-interpretation', 'abstract-machine', 'access-control', 'access-method', 'access-network', 'accounting', 'active-appearance-model', 'active-database', 'active-networking', 'active-shape-model', 'activemq', 'activity-recognition', 'actuarial-science', 'actuator', 'adaboost', 'adaptive-routing', 'adaptive-system', 'adder', 'adobe-illustrator', 'adobe-photoshop', 'advertising', 'aerial-photography', 'aeronautics', 'aerospace-engineering', 'aerospike', 'agile-project-management', 'agricultural-engineering', 'airflow', 'airtable', 'ajax', 'akamai', 'akka', 'algolia', 'algorithm', 'algorithm-design', 'alpine-linux', 'amazon-api-gateway', 'amazon-athena', 'amazon-cloudfront', 'amazon-cloudwatch', 'amazon-cognito', 'amazon-dynamodb', 'amazon-ebs', 'amazon-ec2', 'amazon-ec2-container-service', 'amazon-eks', 'amazon-elasticache', 'amazon-elasticsearch-service', 'amazon-emr', 'amazon-kinesis', 'amazon-kinesis-firehose', 'amazon-machine-learning', 'amazon-rds', 'amazon-rds-for-aurora', 'amazon-rds-for-postgresql', 'amazon-redshift', 'amazon-route-53', 'amazon-s3', 'amazon-ses', 'amazon-sns', 'amazon-sqs', 'amazon-vpc', 'ambiguity', 'amphp', 'amplitude', 'analog-to-digital-converter', 'analysis-of-algorithms', 'analysis-of-covariance', 'analysis-of-variance', 'analytics', 'analytics-integrator', 'android', 'android-sdk', 'android-studio', 'angular', 'angular-2', 'angularjs', 'angularui', 'anomaly-detection', 'ansible', 'ant-design', 'apache-ant', 'apache-cordova', 'apache-flink', 'apache-http-server', 'apache-maven', 'apache-mesos', 'apache-spark', 'apache-tomcat', 'apache-zeppelin', 'api', 'api-documentation-browser', 'api-tools', 'api.ai', 'apiary', 'apigee', 'apollo', 'appium', 'application-and-data', 'application-hosting', 'application-programming-interface', 'application-server', 'application-specific-integrated-circuit', 'application-utilities', 'appveyor', 'arangodb', 'arbol', 'arboriculture', 'arborist', 'arch-linux', 'architectural-engineering', 'arduino', 'art--architecture-thesaurus', 'artificial-intelligence', 'artificial-neural-network', 'asana', 'asp.net', 'aspect-oriented-programming', 'aspnet', 'assets-and-media', 'association-rule-learning', 'asynchronous-communication', 'asynchronous-transfer-mode', 'atom', 'audio-signal', 'augmented-reality', 'aurelia', 'auth0', 'authentication', 'authy', 'automated-reasoning', 'automated-theorem-proving', 'automatic-programming', 'automatic-summarization', 'automatic-taxonomy-induction', 'automaton', 'automotive-engineering', 'autonomic-computing', 'ava', 'awesome', 'aws', 'aws-cloudformation', 'aws-codebuild', 'aws-codecommit', 'aws-codedeploy', 'aws-codepipeline', 'aws-direct-connect', 'aws-elastic-beanstalk', 'aws-elastic-load-balancing-elb', 'aws-fargate', 'aws-iam', 'aws-lambda', 'aws-opsworks', 'awx', 'azure', 'azure-cosmos-db', 'azure-functions', 'azure-machine-learning', 'azure-storage', 'azure-websites', 'babel', 'back-office', 'backbone.js', 'background-processing', 'background-subtraction', 'bag-of-words-model', 'bamboo', 'base-station', 'bash', 'batch-processing', 'bayesian-inference', 'bayesian-network', 'bayesian-probability', 'bazel', 'beanstalk', 'beanstalkd', 'benchmark-computing', 'beta-by-crashlytics', 'beta-testing--mobile-app-distribution', 'bibliographic-database', 'bibliometrics', 'big-data', 'big-data-as-a-service', 'big-data-tools', 'binary-search-tree', 'biochemical-engineering', 'biological-database', 'biomedical-engineering', 'biometrics', 'bitbucket', 'bitcoin', 'bitrise', 'blind-signal-separation', 'blockchain', 'body-text', 'boolean-algebra', 'boolean-expression', 'boosting-machine-learning', 'boot2docker', 'bootstrap', 'bootswatch', 'bot', 'bourbon', 'bower', 'box', 'brackets', 'brain-mapping', 'braintree', 'branch-and-bound', 'breadth-first-search', 'brightness', 'broadcasting', 'browser-testing', 'browserify', 'browserstack', 'browsersync', 'brunch', 'brute-force-search', 'buddy', 'buffer', 'buffer-overflow', 'build-test-deploy', 'buildkite', 'built-in-self-test', 'bulma', 'business', 'business-administration', 'business-dashboards', 'business-intelligence', 'business-process', 'business-tools', 'c', 'c3.js', 'caddy', 'cakephp', 'canonical-correlation', 'canonical-model', 'capistrano', 'capybara', 'case-based-reasoning', 'cassandra', 'cataloging', 'cdnjs', 'celery', 'cellular-network', 'centos', 'central-processing-unit', 'cepstrum', 'certificate', 'certificate-authority', 'change-detection', 'chartbeat', 'charting-libraries', 'chatbot-platforms--tools', 'chatops', 'chef', 'chemical-engineering', 'chemometrics', 'chrome', 'chrome-extension', 'circleci', 'circumscription', 'civil-engineering', 'classical-logic', 'classifier-linguistics', 'clef', 'clever-cloud', 'cli', 'clicktale', 'clicky', 'clientserver-model', 'clinicalkey', 'clion', 'clojure', 'clojurescript', 'clone-java-method', 'cloud-access-management', 'cloud-content-management-system', 'cloud-firestore', 'cloud-foundry', 'cloud-functions-for-firebase', 'cloud-hosting', 'cloud-ide', 'cloud-monitoring', 'cloud-storage', 'cloud9-ide', 'cloudflare', 'cloudinary', 'clubhouse', 'cluster-analysis', 'cluster-management', 'cobol', 'cocoa-touch-ios', 'codacy', 'code-climate', 'code-collaboration--version-control', 'code-coverage', 'code-generation', 'code-quality', 'code-review', 'codeanywhere', 'codebook', 'codec', 'codecov', 'codeigniter', 'codekit', 'codemirror', 'codenvy', 'codeship', 'coding-social-sciences', 'coffeescript', 'cognitive-neuroscience-of-visual-object-recognition', 'collaboration', 'collision', 'color-vision', 'colorimetry', 'combinatorial-optimization', 'commenting-service', 'commerce', 'common-object-request-broker-architecture', 'communication-complexity', 'communication-in-small-groups', 'communications', 'communications-protocol', 'communications-sdk', 'communications-system', 'compass', 'compiler', 'complex-data-type', 'complex-system', 'component', 'component-analysis', 'compose', 'composer', 'composite-index', 'composite-number', 'comprehension', 'compressed-sensing', 'computability', 'computability-theory', 'computation', 'computational-complexity-theory', 'computational-geometry', 'computational-mathematics', 'computational-model', 'computational-science', 'computer-aided-software-engineering', 'computer-architecture', 'computer-cluster', 'computer-data-storage', 'computer-engineering', 'computer-file', 'computer-graphics', 'computer-graphics-images', 'computer-hardware', 'computer-multitasking', 'computer-network', 'computer-program', 'computer-programming', 'computer-science', 'computer-security', 'computer-vision', 'concept-learning', 'conceptual-model', 'concourse', 'concurrency', 'concurrency-control', 'concurrency-frameworks', 'concurrent-computing', 'conditional-random-field', 'conductor', 'confirmatory-factor-analysis', 'confluence', 'consistency-model', 'constant-false-alarm-rate', 'construction-engineering', 'consul', 'container-tools', 'containers-as-a-service', 'content-delivery-network', 'content-management', 'context-free-language', 'contextual-query-language', 'contingency-table', 'continuous-deployment', 'continuous-integration', 'continuum-design-consultancy', 'control-engineering', 'control-flow', 'control-reconfiguration', 'control-theory', 'controlled-vocabulary', 'conventional-pci', 'convergence-routing', 'coordinate-system', 'copy-protection', 'core-network', 'coreos', 'corner-detection', 'correctness', 'correlation-coefficient', 'cost-database', 'couchbase', 'couchdb', 'coveralls', 'cpp', 'cranfield-experiments', 'crashlytics', 'crazy-egg', 'create-react-app', 'create-react-native-app', 'critical-mass-software-engineering', 'critical-path-method', 'critical-section', 'crm', 'cross-correlation', 'cross-platform-desktop-development', 'cross-platform-mobile-development', 'cross-platform-mobile-tools', 'cross-validation', 'crowdsourcing', 'cryptocurrency', 'cryptographic-protocol', 'cryptography', 'crystal', 'csharp', 'css', 'css-pre-processors--extensions', 'cucumber', 'curse-of-dimensionality', 'curve-fitting', 'custom-analytics', 'customer-analytics', 'customer-relationship-management', 'customer-support-chat', 'cypress', 'd3.js', 'dart', 'dash', 'data-access', 'data-acquisition', 'data-analysis', 'data-as-a-service', 'data-center', 'data-classification', 'data-compression', 'data-consistency', 'data-cube', 'data-exchange', 'data-extraction', 'data-file', 'data-flow-diagram', 'data-integration', 'data-integrity', 'data-logger', 'data-management', 'data-manipulation-language', 'data-mining', 'data-model', 'data-modeling', 'data-pre-processing', 'data-processing', 'data-quality', 'data-reduction', 'data-retrieval', 'data-science', 'data-science-notebooks', 'data-science-tools', 'data-sharing', 'data-stores', 'data-stream', 'data-stream-mining', 'data-structure', 'data-structures', 'data-system', 'data-transmission', 'data-type', 'data-validation', 'data-visualization', 'data-warehouse', 'database', 'database-design', 'database-security', 'database-tools', 'database-transaction', 'databases', 'datadog', 'datalog', 'dcos', 'deadlock', 'debian', 'debugging', 'decidability', 'decision-problem', 'decision-rule', 'decision-support-system', 'decision-tree', 'deco', 'decomposition-method-constraint-satisfaction', 'dedicated-cloud-hosting', 'deductive-database', 'deep-learning', 'dempstershafer-theory', 'denial-of-service-attack', 'deontic-logic', 'dependency-management', 'dependency-monitoring', 'deploybot', 'deployment', 'deployment-as-a-service', 'description-logic', 'design', 'design-for-testing', 'detection-theory', 'deterministic-automaton', 'devdocs', 'devise', 'devops', 'diagram', 'digital-filter', 'digital-radio', 'digital-signal-processing', 'digital-signature', 'digital-subscriber-line', 'digitalocean', 'dijkstras-algorithm', 'dimensionality-reduction', 'directed-graph', 'discrete-cosine-transform', 'discrete-event-simulation', 'discrete-logarithm', 'discrete-system', 'discriminative-model', 'disjunctive-normal-form', 'display-device', 'disqus', 'distance-transform', 'distributed-algorithm', 'distributed-computing', 'distributed-computing-environment', 'distributed-data-store', 'distributed-database', 'distributed-file-system', 'distributed-memory', 'distributed-object', 'divide-and-conquer-algorithms', 'django', 'django-rest-framework', 'dns-management', 'dnsimple', 'docker', 'docker-cloud', 'docker-compose', 'docker-for-aws', 'docker-machine', 'docker-swarm', 'doctrine-2', 'document-classification', 'document-collaboration', 'document-layout-analysis', 'document-management-system', 'document-processing', 'document-retrieval', 'document-signature', 'documentation', 'documentation-as-a-service--tools', 'dokku', 'domain-knowledge', 'domain-model', 'domain-registration', 'dotnet', 'drone.io', 'dropbox', 'dropwizard', 'druid', 'drupal', 'duplicate-content', 'durability', 'dyn', 'dynamic-data', 'dynamic-loading', 'dynamic-programming', 'dynamic-range', 'dynamic-source-routing', 'dynamic-testing', 'dynamic-time-warping', 'echo', 'eclipse', 'ecommerce', 'econometric-model', 'economic-policy', 'edit-distance', 'elasticsearch', 'electrical-engineering', 'electroencephalography', 'electron', 'electronic-data-interchange', 'electronic-document', 'electronic-engineering', 'elixir', 'elm', 'emacs', 'email-marketing', 'email-testing', 'embedded-system', 'ember', 'ember.js', 'emoji', 'emotion', 'emotion-recognition', 'emulator', 'encoding-memory', 'encryption', 'engagementlifecycle-marketing', 'engineering', 'engineering-drawing', 'engineering-ethics', 'ensemble-learning', 'enterprise-information-security-architecture', 'enterprise-system', 'entityrelationship-model', 'environmental-engineering', 'enzyme', 'erasure-code', 'erlang', 'error-concealment', 'error-detection-and-correction', 'errors-in-variables-models', 'es6', 'eslint', 'espace', 'etcd', 'ethereum', 'ethernet', 'euclidean-distance', 'eureka', 'evolutionary-algorithm', 'exact-algorithm', 'exception-monitoring', 'expander-graph', 'expectationmaximization-algorithm', 'expert-system', 'exploratory-data-analysis', 'expressjs', 'external-data-representation', 'eye-movement', 'eye-tracking', 'f', 'fabric', 'fabric-by-twitter', 'faceted-classification', 'faceted-search', 'facial-expression', 'facial-recognition-system', 'factorial-experiment', 'false-positive-rate', 'fast-fourier-transform', 'fastlane', 'fastly', 'fault-model', 'fault-tolerance', 'feathersjs', 'feature-detection', 'feature-extraction', 'feature-selection', 'feature-vector', 'fedora', 'ffmpeg', 'field-of-view', 'field-programmable-gate-array', 'figma', 'figure-of-merit', 'file-format', 'file-storage', 'file-system', 'file-uploads', 'filestack', 'filter-signal-processing', 'finagle', 'finance', 'financial-system', 'findability', 'fingerprint', 'fingerprint-recognition', 'finite-state-machine', 'firebase', 'firefox', 'first-class', 'first-order-logic', 'flash-memory', 'flask', 'flat-panel-display', 'flip-flop', 'floating-point', 'flow-control-data', 'flow-type', 'fluentd', 'flurry', 'flutter', 'flux', 'flyway', 'folksonomy', 'font', 'forensic-engineering', 'formal-concept-analysis', 'formal-language', 'formal-methods', 'formal-specification', 'formal-verification', 'fortran', 'forums', 'fossa', 'fragmentation-computing', 'framer', 'framework', 'framework7', 'frameworks-full-stack', 'frequency-analysis', 'frequency-domain', 'front-and-back-ends', 'front-end-frameworks', 'front-end-package-manager', 'frontend', 'fullstory', 'functional-dependency', 'functional-programming', 'functional-testing', 'funnel-analysis-analytics', 'fusion', 'fuzzy-logic', 'g-suite', 'game-development', 'game-engine', 'garbage-collection', 'gatling', 'gatsby', 'gauges', 'gaussian-noise', 'gaussian-process', 'gearman', 'geckoboard', 'general-analytics', 'generalization-error', 'generalized-linear-model', 'genetic-algorithm', 'geographic-information-retrieval', 'geometric-modeling', 'geoparsing', 'geospatial-analysis', 'geotagging', 'geotechnical-engineering', 'gerrit-code-review', 'gist', 'git', 'git-tools', 'gitbucket', 'github', 'github-api', 'github-enterprise', 'github-pages', 'gitkraken', 'gitlab', 'gitlab-ci', 'gitlab-pages', 'gluon', 'gnu-bash', 'go', 'go.cd', 'godaddy', 'gogs', 'goodness-of-fit', 'google', 'google-analytics', 'google-app-engine', 'google-app-maker', 'google-bigquery', 'google-cloud-bigtable', 'google-cloud-container-builder', 'google-cloud-dataflow', 'google-cloud-datastore', 'google-cloud-dns', 'google-cloud-functions', 'google-cloud-memorystore', 'google-cloud-messaging', 'google-cloud-pubsub', 'google-cloud-sql', 'google-cloud-storage', 'google-cloud-vision-api', 'google-compute-engine', 'google-drive', 'google-kubernetes-engine', 'google-maps', 'google-scholar-and-academic-libraries', 'google-sheets', 'google-tag-manager', 'gradient-descent', 'gradle', 'grafana', 'grails', 'grape', 'graph-databases', 'graphic-design', 'graphical-model', 'graphical-user-interface', 'graphite', 'graphql', 'greedy-algorithm', 'groovy', 'ground-truth', 'group-chat--notifications', 'growbag', 'grpc', 'grunt', 'gsm', 'gtp', 'gulp', 'gunicorn', 'guzzle', 'hadoop', 'haml', 'handlebars.js', 'handover', 'hapi', 'haproxy', 'haptic-technology', 'hash-function', 'haskell', 'hasura', 'hazelcast', 'hbase', 'headless-browsers', 'heap', 'heap-data-structure', 'heatmap-analytics', 'helm', 'help-desk', 'heroku', 'heroku-ci', 'heroku-postgres', 'heroku-redis', 'hetzner-online-ag', 'hexo', 'hhvm-hiphop-virtual-machine', 'hibernate', 'hidden-markov-model', 'hierarchical-database-model', 'high-availability', 'high-level-programming-language', 'high-level-synthesis', 'highcharts', 'higher-order-statistics', 'hilberthuang-transform', 'hipchat', 'histogram', 'hockeyapp', 'hogan.js', 'homebrew', 'homebridge', 'homogeneity-statistics', 'hosted-blogging-platforms', 'hotjar', 'hough-transform', 'html', 'html5', 'http', 'hubspot', 'hugo', 'human-visual-system-model', 'humancomputer-information-retrieval', 'humancomputer-interaction', 'hybrid-algorithm', 'hybrid-system', 'hypercube', 'hyperspectral-imaging', 'ibm---api-connect', 'ibm-db2', 'icon-font', 'iframely', 'ifttt', 'image-analysis-api', 'image-fusion', 'image-meta-search', 'image-processing', 'image-processing-and-management', 'image-quality', 'image-registration', 'image-resolution', 'image-retrieval', 'image-segmentation', 'image-sensor', 'imgix', 'immutable.js', 'impala', 'in-memory-databases', 'incapsula', 'incremental-learning', 'indentation', 'independent-component-analysis', 'independent-set', 'index-term', 'industrial-engineering', 'industrial-organization', 'inference', 'influxdb', 'infobox', 'information-discovery', 'information-extraction', 'information-filtering-system', 'information-flow-information-theory', 'information-integration', 'information-management', 'information-model', 'information-overload', 'information-retrieval', 'information-retrieval-applications', 'information-retrieval-query-language', 'information-seeking', 'information-theory', 'infrastructure-build-tools', 'inkwell', 'input-device', 'insomnia-rest-client', 'inspec', 'instrumental-variable', 'integer-programming', 'integrated-circuit-design', 'integrated-development-environment', 'integrated-development-environment-tools', 'integrated-services-digital-network', 'intelligent-agent', 'intelligent-document', 'intelligent-network', 'intellij-idea', 'inter-process-communication', 'interaction', 'interactive-mockups', 'intercom', 'interconnection', 'international-trade', 'internet-of-things', 'internet-of-things-hardware', 'internet-privacy', 'internet-protocol-suite', 'internetworking', 'interpolation', 'intrusion-detection-system', 'intuitionistic-logic', 'invision', 'ionic', 'ios', 'ipfs', 'ir-evaluation', 'issue-tracking', 'istio', 'iterative-reconstruction', 'jasmine', 'java', 'java-build-tools', 'javascript', 'javascript-framework-components', 'javascript-mvc-frameworks', 'javascript-testing-framework', 'javascript-ui-libraries', 'javascript-utilities--libraries', 'jekyll', 'jenkins', 'jest', 'jetty', 'jira', 'jitter', 'jquery', 'jquery-mobile', 'jquery-ui', 'jruby', 'js-build-tools--js-task-runners', 'jsdoc', 'json', 'json-server', 'julia', 'junit', 'jupyter', 'jupyter-notebook', 'k-d-tree', 'k-nearest-neighbors-algorithm', 'kafka', 'kalman-filter', 'kanban-for-github-issues', 'kanban-tool', 'karhunenlove-theorem', 'karma', 'kendo-ui', 'keras', 'kernel-linear-algebra', 'key-exchange', 'key-lock', 'keycdn', 'keyword-extraction', 'kibana', 'kissmetrics', 'kitematic', 'knapsack-problem', 'knex.js', 'knockoutjs', 'knowledge-acquisition', 'knowledge-base', 'knowledge-based-systems', 'knowledge-extraction', 'knowledge-management', 'knowledge-modeling', 'knowledge-representation-and-reasoning', 'knowledge-retrieval', 'koa', 'koding', 'kong', 'kotlin', 'kubernetes', 'kullbackleibler-divergence', 'lambda-calculus', 'lambdatest', 'landing-pages', 'language-acquisition', 'language-model', 'languages', 'languages--frameworks', 'laravel', 'laravel-forge', 'laravel-homestead', 'lastpass', 'latency-engineering', 'latent-dirichlet-allocation', 'latent-semantic-indexing', 'latex', 'layout-engine', 'leaflet', 'least-squares', 'legal-information-retrieval', 'less', 'lets-encrypt', 'level-of-detail', 'level-set', 'lexico', 'libraries', 'library', 'library-science', 'line-of-sight', 'linear-discriminant-analysis', 'linear-logic', 'linear-model', 'linear-prediction', 'linear-programming', 'linear-regression', 'linear-search', 'linear-temporal-logic', 'link-analysis', 'link-layer', 'link-relation', 'linked-data', 'linode', 'linux', 'linux-mint', 'liquibase', 'liquid-crystal-display', 'live-reloading', 'livereload', 'load-and-performance-testing', 'load-balancer--reverse-proxy', 'load-balancing-computing', 'load-management', 'loader.io', 'local-area-network', 'local-search-optimization', 'localhost-tools', 'localization', 'location-based-service', 'locust', 'lodash', 'log-management', 'loggly', 'logic-gate', 'logic-in-computer-science', 'logic-programming', 'logic-synthesis', 'logical-framework', 'logistic-regression', 'logrocket', 'logstash', 'look-ahead', 'lookup-table', 'lottie', 'low-pass-filter', 'lua', 'lucene', 'lumen', 'lxc', 'lxd', 'machine-learning', 'machine-learning-as-a-service', 'machine-learning-tools', 'machine-vision', 'macos', 'magento', 'mailchimp', 'mailgun', 'mailjet', 'managed-memcache', 'management-science', 'mandrill', 'manufacturing-engineering', 'mapbox', 'mapping-apis', 'marathon', 'mariadb', 'marine-engineering', 'marionette', 'markdown', 'marketing', 'marketing-automation', 'markov-chain', 'markup-language', 'mastodon', 'matched-filter', 'material', 'material-design', 'material-design-for-angular', 'material-design-for-bootstrap', 'material-design-lite', 'material-ui', 'materialize', 'mathematical-logic', 'matlab', 'mattermost', 'maven', 'maxcdn', 'maximum-a-posteriori-estimation', 'mean', 'mean-shift', 'mechanical-engineering', 'media-access-control', 'medical-imaging', 'medical-literature-retrieval', 'medium', 'memcached', 'memcachier', 'memory-management', 'mercurial', 'mesh-networking', 'mesosphere', 'message-passing', 'message-queue', 'messenger-platform', 'metabase', 'metadata', 'metamodeling', 'meteor', 'metis', 'microcomputer', 'microcontroller', 'microdata-html', 'microframeworks-backend', 'microprocessor', 'microservices-tools', 'microsoft-azure', 'microsoft-bot-framework', 'microsoft-iis', 'microsoft-sql-server', 'middleman', 'middleware', 'mina', 'minecraft', 'minification', 'mining-engineering', 'minio', 'missing-data', 'mixed-model', 'mixpanel', 'mixture-model', 'ml-kit', 'mobile', 'mobile-agent', 'mobile-analytics', 'mobile-backend', 'mobile-computing', 'mobile-continuous-integration', 'mobile-database', 'mobile-error-monitoring', 'mobile-prototyping--interaction-design-tools', 'mobile-push-messaging', 'mobile-radio', 'mobile-robot', 'mobile-station', 'mobile-telephony', 'mobile-testing-frameworks', 'mobile-ui-frameworks', 'mobility-management', 'mobility-model', 'mobx', 'mocha', 'modal-logic', 'mode', 'model-based-reasoning', 'model-checking', 'modular-design', 'momentjs', 'monero', 'mongodb', 'mongodb-atlas', 'mongodb-hosting', 'mongodb-stitch', 'mongoid', 'mongolab', 'mongoose', 'monitoring', 'monitoring-aggregation', 'monitoring-tools', 'morphology-linguistics', 'motion-analysis', 'motion-compensation', 'motion-detection', 'motion-estimation', 'motion-planning', 'moving-average', 'multi-agent-system', 'multi-core-processor', 'multi-objective-optimization', 'multi-user', 'multicast', 'multidimensional-analysis', 'multidimensional-scaling', 'multilevel-model', 'multimedia', 'multimedia-database', 'multimodal-search', 'multiprocessing', 'multisearch', 'multispectral-image', 'multivariate-analysis', 'multivariate-statistics', 'mustache', 'mutual-exclusion', 'mutual-information', 'mvc-tools', 'mvvmcross', 'mysql', 'naive-bayes-classifier', 'namecheap', 'named-entity', 'nativescript', 'nats', 'natural-deduction', 'natural-language', 'natural-language-processing', 'natural-language-understanding', 'navigation-system', 'neo4j', 'neovim', 'netbeans-ide', 'netlify', 'netty', 'network-architecture', 'network-congestion', 'network-delay', 'network-interface', 'network-layer', 'network-management', 'network-model', 'network-performance', 'network-planning-and-design', 'network-security', 'network-simulation', 'network-topology', 'neural-coding', 'new-relic', 'nexmo', 'next-generation-network', 'next.js', 'nginx', 'ngrok', 'nightwatchjs', 'nim', 'nitrous.io', 'node-networking', 'node.js', 'node.js-process-manager', 'nodejs', 'noise-measurement', 'noise-reduction', 'noisy-data', 'nomad', 'non-volatile-memory', 'nosql', 'nosql-database-as-a-service', 'notepad', 'npm', 'nsq', 'nuclear-engineering', 'numerical-stability', 'numpy', 'nuxt', 'object-detection', 'object-document-mapper-odm', 'object-model', 'object-oriented-programming', 'object-relational-mapper-orm', 'objective-c', 'octodns', 'octopus-deploy', 'omniauth', 'onesignal', 'online-algorithm', 'online-public-access-catalog', 'onsen-ui', 'ontology-information-science', 'open-data', 'open-postgresql-monitoring', 'open-source-cloud', 'open-source-service-discovery', 'opencv', 'opengl', 'openlayers', 'openresty', 'openshift', 'openstack', 'opensuse', 'operating-system', 'operating-systems', 'operational-transformation', 'operations-management', 'operations-research', 'operator-computer-programming', 'optical-character-recognition', 'optical-disc', 'optical-flow', 'optical-imaging', 'optical-recording', 'optical-transfer-function', 'optimization-problem', 'optimizely', 'oracle', 'outlier', 'overlay-network', 'ovh', 'oxygene', 'p-system', 'p2p', 'package-manager', 'package-managers', 'packer', 'pandas', 'parallel-algorithm', 'parallel-computing', 'parcel', 'parse', 'parse-server', 'parsing', 'particle-filter', 'particle-swarm-optimization', 'passenger', 'passive-optical-network', 'password-management', 'patent-classification', 'patent-visualisation', 'path-analysis-statistics', 'pattern-matching', 'pattern-recognition', 'pattern-recognition-psychology', 'paw', 'payment-services', 'payments', 'paypal', 'peak-signal-to-noise-ratio', 'peer-to-peer', 'performance-metric', 'performance-monitoring', 'performance-prediction', 'perl', 'perl6', 'persistence-computer-science', 'petri-net', 'petroleum-engineering', 'phabricator', 'phalcon', 'phantomjs', 'phaser', 'phoenix-framework', 'phonegap', 'photogrammetry', 'php', 'php-mvc', 'phpstorm', 'phpunit', 'pico-8', 'pile', 'pingdom', 'piwik', 'pixel', 'pixel-art', 'plagiarism-detection', 'platform-as-a-service', 'platform-as-a-service-tools', 'play', 'plotly', 'point-location', 'point-spread-function', 'point-to-point', 'polymer', 'polymorphism-computer-science', 'port-computer-networking', 'portainer', 'pose', 'postcss', 'postgis', 'postgresql', 'postgresql-as-a-service', 'postman', 'postmark', 'pouchdb', 'power-control', 'power-management', 'preact', 'precision-and-recall', 'predictive-coding', 'predictive-value-of-tests', 'presentation-semantics', 'prestashop', 'presto', 'principal-component-analysis', 'prisma', 'process-calculus', 'process-engineering', 'process-management', 'prognostics', 'program-analysis', 'program-design-language', 'program-optimization', 'program-synthesis', 'program-transformation', 'programmable-logic-device', 'programming--code-analytics', 'programming-language', 'programming-paradigm', 'project-management', 'prolog', 'prometheus', 'propagation-delay', 'propositional-calculus', 'prosthesis', 'protractor', 'proximity-search', 'publishing', 'pubnub', 'pug', 'pulp-and-paper-industry', 'pulse-signal-processing', 'puma', 'puppet-labs', 'puppeteer', 'push-monkey', 'pushdown-automaton', 'pusher', 'pushwoosh', 'pwa', 'pycharm', 'python', 'pytorch', 'qt', 'quality-of-service', 'quantifier-elimination', 'quantization-signal-processing', 'quantum-algorithm', 'quantum-computer', 'query-expansion', 'query-language', 'query-languages', 'query-optimization', 'query-string', 'question-answering', 'queue', 'queueing-theory', 'qunit', 'r', 'rabbitmq', 'rackspace-cloud-servers', 'rails', 'rails-api', 'raml', 'rancher', 'random-access', 'random-effects-model', 'random-forest', 'random-indexing', 'random-projection', 'range-query-data-structures', 'ranking', 'raspberry-pi', 'ratchet', 'rdf', 'reachability', 'react', 'react-hot-loader', 'react-native', 'react-navigation', 'react-router', 'react-storybook', 'react.js-boilerplate', 'reactiveui', 'read-only-memory', 'read-write-memory', 'readme.io', 'real-time-communication', 'real-time-computing', 'real-time-data', 'real-time-data-processing', 'real-time-operating-system', 'realm', 'realtime-analytics', 'realtime-backend--api', 'receiver-operating-characteristic', 'recommender-system', 'record-linkage', 'recurly', 'recursion', 'redash', 'redis', 'redis-cloud', 'redis-hosting', 'redmine', 'redundancy-engineering', 'redux', 'redux-saga', 'redux-thunk', 'redux.js', 'reference-frame', 'region-of-interest', 'regression-analysis', 'regular-expression', 'regular-language', 'reinforcement-learning', 'relational-database', 'relative-record-data-set', 'relay', 'release', 'reliability-computer-networking', 'reliability-engineering', 'remote-control', 'remote-procedure-call', 'remote-procedure-call-rpc', 'replication-computing', 'requirejs', 'residual', 'resource-allocation', 'response-surface-methodology', 'response-time', 'resque', 'rest-api', 'result-set', 'rethinkdb', 'reverse-engineering', 'riak', 'riot', 'risk-analysis-engineering', 'robot', 'robustness-computer-science', 'rocket', 'rollbar', 'rotation', 'rough-set', 'round-trip-delay-time', 'router', 'routing-protocol', 'rspec', 'rubocop', 'ruby', 'rubymine', 'rule-based-system', 'rule-induction', 'rule-of-thumb', 'runscope', 'runtime-system', 'rust', 'rxjs', 'sails.js', 'salesforce-sales-cloud', 'sass', 'satisfiability', 'sauce-labs', 'scaffold', 'scala', 'scalability', 'scalable-vector-graphics', 'scale-space', 'scaleway', 'scanner', 'schedule', 'scheduling-computing', 'scientometrics', 'scikit-learn', 'screen-sharing', 'scripting-language', 'scrutinizer', 'sdn', 'search-algorithm', 'search-as-a-service', 'search-box', 'search-engine', 'search-engine-indexing', 'search-engines', 'searchretrieve-via-url', 'seasonality', 'secret-sharing', 'secrets-management', 'secure-communication', 'secure-multi-party-computation', 'security', 'seesaw', 'segment', 'segmentation', 'selenium', 'self-hosted-blogging--cms', 'self-management', 'self-organization', 'semantic-computing', 'semantic-data-model', 'semantic-html', 'semantic-matching', 'semantic-network', 'semantic-similarity', 'semantic-ui', 'semantic-ui-react', 'semantic-web', 'semantic-web-stack', 'semantics', 'semaphore', 'semi-supervised-learning', 'sencha-touch', 'sendbird', 'sendgrid', 'sendwithus', 'sensor-array', 'sensor-fusion', 'sensor-tower', 'sentiment-analysis', 'sentry', 'sequel-pro', 'sequelize', 'sequential-logic', 'sequential-pattern-mining', 'server', 'server-configuration-and-automation', 'serverless', 'serverless--task-processing', 'shape-analysis-digital-geometry', 'shared-memory', 'shared-resource', 'shell', 'shields.io', 'shift-register', 'shippable', 'shopify', 'shortest-path-problem', 'shrinkage', 'sidekiq', 'signal-processing', 'signal-strength', 'signalr', 'similarity-measure', 'simulated-annealing', 'simulation', 'sinatra', 'skeleton', 'skeleton-computer-programming', 'sketch', 'skype', 'slack', 'sliding-window-protocol', 'slim', 'slim-lang', 'sma', 'smart-card', 'smart-information-retrieval-system', 'smoothing', 'snippet', 'snowflake', 'snyk', 'social-media-tools', 'socket.io', 'softlayer', 'software', 'software-agent', 'software-defined-radio', 'software-engineering', 'solid-modeling', 'solr', 'sonar', 'sonarqube', 'sonatype-nexus', 'sorting', 'source-code', 'source-code-management-desktop-apps', 'source-document', 'source-separation', 'sourcetree', 'spacemacs', 'spacevim', 'sparkpost', 'sparse-approximation', 'spatial-analysis', 'spatial-frequency', 'speaker-recognition', 'specification', 'specification-language', 'speckle-pattern', 'spectrogram', 'speech-coding', 'speech-processing', 'speech-recognition', 'sphinx', 'spread-spectrum', 'spreadsheets-as-a-backend', 'spreadsheets-online', 'spree', 'spring', 'spring-boot', 'spring-cloud', 'sql', 'sql-database-as-a-service', 'sqlalchemy', 'sqlite', 'squarespace', 'stack-overflow', 'stackdriver', 'stamplay', 'standard-ml', 'stars', 'state-diagram', 'state-management-library', 'static-random-access-memory', 'static-site-generators', 'static-timing-analysis', 'static-web-hosting', 'statistical-classification', 'statistical-model', 'status-page-hosting', 'statuscake', 'stemming', 'stereopsis', 'stitch', 'stop-words', 'storm', 'storybook', 'stream-processing', 'streams', 'stripe', 'strips', 'strongly-connected-component', 'structural-engineering', 'structural-equation-modeling', 'structured-document', 'structured-text', 'styled-components', 'stylelint', 'stylus', 'subject-access', 'sublime-text', 'supercomputer', 'superresolution', 'superset', 'supervised-learning', 'supervisory-control', 'support', 'support-sales-and-marketing', 'support-vector-machine', 'surge', 'survey-widget', 'svn-subversion', 'swagger-ui', 'swift', 'swiftype', 'symfony', 'symmetric-multiprocessor-system', 'synchronization', 'syntax', 'synthetic-aperture-radar', 'synthetic-data', 'system-integration', 'system-on-a-chip', 'system-testing', 'systems-engineering', 'systems-management', 'table-information', 'tableau', 'tabu-search', 'tailwind-css', 'task-management', 'teamcity', 'technological-change', 'telecommunications', 'telecommunications-network', 'telecommunications-service', 'telegram', 'telegram-bot-api', 'telephony', 'template-matching', 'templating-languages--extensions', 'temporal-database', 'tensorflow', 'terminal', 'terraform', 'testflight', 'testing', 'testing-frameworks', 'text-box', 'text-editor', 'text-mining', 'text-processing', 'text-retrieval-conference', 'text-segmentation', 'textmate', 'tfidf', 'theoretical-computer-science', 'thread-computing', 'thresholding', 'throughput', 'time-constraint', 'time-domain', 'time-of-arrival', 'time-series', 'time-sharing', 'time-to-market', 'time-tracking', 'timefrequency-analysis', 'title-search', 'tools-for-github', 'tools-for-text-editors', 'top-down-and-bottom-up-design', 'topic-maps', 'topic-model', 'tornado', 'tower', 'tracking-system', 'traefik', 'traffic-engineering', 'traffic-model', 'training-set', 'transactional-email', 'transition-system', 'translation-service', 'transparency-graphic', 'transport-engineering', 'transport-layer', 'travelling-salesman-problem', 'travis-ci', 'trecvid', 'tree-automaton', 'tree-data-structure', 'tree-structure', 'trello', 'trend-analysis', 'triangulation-social-science', 'turing-machine', 'twilio', 'twitter', 'type-inference', 'type-theory', 'typeform', 'typescript', 'ubiquitous-computing', 'ubuntu', 'uglifyjs', 'uikit', 'uncertain-data', 'underscore', 'unicorn', 'unified-medical-language-system', 'unified-modeling-language', 'unity', 'unreal-engine', 'unsupervised-learning', 'uppy', 'uptime-robot', 'urban-airship', 'user-feedback-as-a-service', 'user-interface', 'user-management-and-authentication', 'user-profile', 'utilities', 'vagrant', 'variables', 'varnish', 'vault', 'vector-space-model', 'version-control-system', 'very-large-database', 'video-processing', 'video-quality', 'video-tracking', 'vim', 'virtual-circuit', 'virtual-machine', 'virtual-machine-management', 'virtual-machine-platforms--containers', 'virtual-memory', 'virtual-organization', 'virtual-private-cloud', 'virtual-reality', 'virtualbox', 'virtualization-platform', 'visual-basic', 'visual-cortex', 'visual-inspection', 'visual-programming-language', 'visual-studio', 'visual-studio-code', 'visual-studio-team-services', 'visualization', 'viterbi-algorithm', 'vmware-vsphere', 'voice-and-sms', 'voice-over-ip', 'vue', 'vue.js', 'vuepress', 'vuetify', 'vuex', 'wagtail', 'wakatime', 'waste-management', 'watershed', 'wavefront', 'wavelength-division-multiplexing', 'wavelet', 'web-and-video-conferencing', 'web-app-builders', 'web-components', 'web-crawler', 'web-document', 'web-forms', 'web-mining', 'web-server', 'web-servers', 'web-service', 'web-service-automation', 'web-starter-kit', 'webapp', 'webflow', 'webpack', 'website-builder', 'website-monitoring', 'webstorm', 'weebly', 'wercker', 'wide-area-network', 'wiener-filter', 'wimax', 'windows', 'wireframing', 'wireless', 'wireless-ad-hoc-network', 'wireless-network', 'wireless-sensor-network', 'wix', 'woocommerce', 'woopra', 'word-error-rate', 'word-sense-disambiguation', 'wordnet', 'wordplate', 'wordpress', 'workflow', 'workflow-manager', 'workload', 'world-wide-web', 'xamarin', 'xcode', 'xml', 'yarn', 'yeoman', 'yii', 'zend-framework', 'zendesk', 'zenefits', 'zeplin', 'zepto', 'zeromq', 'zoho-crm', 'zookeeper']\n"
     ]
    }
   ],
   "source": [
    "# Load skills from the patterns file (assuming itâ€™s a JSONL file)\n",
    "# Load skills from the provided JSON file\n",
    "def load_skills_from_json(skills_file):\n",
    "    skills_list = []\n",
    "    with open(skills_file, 'r') as f:\n",
    "        data = json.load(f)  # Load the entire JSON file\n",
    "        for skill in data.keys():  # The keys at the top level are the skill names\n",
    "            skills_list.append(skill)  # Add the skill to the list\n",
    "    return skills_list\n",
    "\n",
    "# Load the skills from your JSON file\n",
    "skills_list = load_skills_from_json('../data/skills.json')\n",
    "\n",
    "# Print the loaded skills\n",
    "print(\"Skills loaded from JSON:\", skills_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1991\n"
     ]
    }
   ],
   "source": [
    "print(len(skills_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_skill_embeddings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Generate embeddings for the extracted skills list\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m skill_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mget_skill_embeddings\u001b[49m(skills_list, model, tokenizer)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Example job description to compare\u001b[39;00m\n\u001b[1;32m      5\u001b[0m job_desc_embedding \u001b[38;5;241m=\u001b[39m get_embeddings(job_description_text, model_name)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_skill_embeddings' is not defined"
     ]
    }
   ],
   "source": [
    "# Step 3: Generate skill embeddings\n",
    "def get_skill_embeddings(skills, model, tokenizer):\n",
    "    skill_embeddings = []\n",
    "    for skill in skills:\n",
    "        inputs = tokenizer(skill, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "        outputs = model(**inputs)\n",
    "        embeddings = outputs.last_hidden_state.mean(dim=1).detach().to(\"cpu\").numpy()\n",
    "        skill_embeddings.append(embeddings)\n",
    "    return np.array(skill_embeddings)\n",
    "\n",
    "# Generate embeddings for the skills\n",
    "skill_embeddings = get_skill_embeddings(skills_list, model, tokenizer)\n",
    "\n",
    "# Step 4: Compute similarity and find top skills for each job description\n",
    "def find_top_skills(job_desc_embedding, skill_embeddings, skills_list, top_n=5):\n",
    "    similarities = cosine_similarity(job_desc_embedding, skill_embeddings).flatten()\n",
    "    top_n_indices = similarities.argsort()[-top_n:][::-1]  # Sort and get top N in descending order\n",
    "    top_skills = [skills_list[i] for i in top_n_indices]\n",
    "    return top_skills\n",
    "\n",
    "# Apply the top skills function to each job description in your DataFrame\n",
    "df['top_skills'] = df['Features'].apply(lambda x: find_top_skills(get_embeddings(x, model_name), skill_embeddings, skills_list))\n",
    "\n",
    "# Show the results\n",
    "print(df[['Features', 'top_skills']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
