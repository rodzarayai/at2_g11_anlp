{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3f0f87e-66c8-4559-b45f-e93646af4c30",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rodzaraya/opt/anaconda3/lib/python3.9/site-packages/spacy/util.py:275: UserWarning: [W031] Model 'en_core_web_sm' (2.2.0) requires spaCy v2.2 and is incompatible with the current spaCy version (2.3.5). This may lead to unexpected results or runtime errors. To resolve this, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "#Text processing and cleaning\n",
    "import contractions # To include english contractions\n",
    "import re #regex\n",
    "import string #used to include punctuation during text processing\n",
    "from collections import Counter #count strings in texts\n",
    "\n",
    "#Natural Language Tool Kit NLK package\n",
    "import nltk\n",
    "from nltk.corpus import stopwords #Stopwords\n",
    "\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b41ca7a-7c1d-4808-9023-7b90b3356db6",
   "metadata": {
    "tags": []
   },
   "source": [
    "All the cell above must be in the aux_func.py file as auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db716670-e047-48cb-97c7-9d5eae90bea9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb4cc0b5-0149-4a77-b536-b472246bdf03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_skills_from_json(skills_file):\n",
    "    skills_list = []\n",
    "    with open(skills_file, 'r') as f:\n",
    "        data = json.load(f)  # Load the entire JSON file\n",
    "        for skill in data.keys():  # The keys at the top level are the skill names\n",
    "            skills_list.append(skill)  # Add the skill to the list\n",
    "    return skills_list\n",
    "\n",
    "\n",
    "#=======================================Processing class\n",
    "class TextPreprocessor:\n",
    "    def __init__(self, processing_mode='none', custom_punctuation=None, custom_stopwords=None, sentence_analysis=False):\n",
    "        \"\"\"\n",
    "            Initialization considers Custom punctuation, Stop words, and Lemmatizer or Stemmer.\n",
    "            Updates custom punctuation and custom stop words set with additional ones if provided.\n",
    "            The processing mode to standardise variants can be choose between none, Stem and Lemma. Each mode is stored in a different\n",
    "            column of the dataframe.\n",
    "            Sentence analysis parameter is used to keep the punctuation symbols required for sentence analysis.\n",
    "\n",
    "            Parameters:\n",
    "            - processing_mode: String to decide whether to use 'lemma', 'stem', or 'none' for text processing.\n",
    "            - custom_punctuation: Additional punctuation characters to remove from text.\n",
    "            - custom_stopwords: Additional stopwords to remove from text.\n",
    "            - sentence_analysis: Boolean indicating sentence analysis cleaning steps. This mode will keep the punctuation symbols.\n",
    "\n",
    "            \"\"\"\n",
    "\n",
    "        self.punctuation = string.punctuation #Init with all punctuation characters\n",
    "\n",
    "        if custom_punctuation:\n",
    "            self.punctuation += custom_punctuation #add custom punctuation\n",
    "\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        if custom_stopwords:\n",
    "            self.stop_words.update(custom_stopwords) #add custom stopwords\n",
    "\n",
    "        # Determine which text processing mode to use\n",
    "        self.processing_mode = processing_mode.lower()\n",
    "\n",
    "        # Set the sentence analysis mode\n",
    "        self.sentence_analysis = sentence_analysis\n",
    "\n",
    "        #Set the variant standardization mode\n",
    "        if self.processing_mode == 'lemma':\n",
    "            self.lemmatizer = WordNetLemmatizer()\n",
    "        elif self.processing_mode == 'stem':\n",
    "            self.stemmer = PorterStemmer()\n",
    "\n",
    "    # Expand contractions using the contractions library\n",
    "    def expand_contractions(self, text):\n",
    "        return contractions.fix(text)\n",
    "\n",
    "    # Split hyphenated words into separate words, like phone numbers or radio fm, age, etc.\n",
    "    def split_hyphenated_words(self, text):\n",
    "        return re.sub(r'-', ' ', text)\n",
    "\n",
    "    def remove_punctuation(self, text):\n",
    "        return ''.join([char for char in text if char not in self.punctuation])\n",
    "\n",
    "    def add_space_after_parenthesis(self, text):\n",
    "        return re.sub(r'\\)', ') ', text)\n",
    "\n",
    "    def to_lowercase(self, text):\n",
    "        return text.lower()\n",
    "\n",
    "    def remove_stopwords(self, text):\n",
    "        words = word_tokenize(text)\n",
    "        return ' '.join([word for word in words if word not in self.stop_words])\n",
    "\n",
    "    def remove_extra_whitespace(self, text):\n",
    "        return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    def stem_words(self, text):\n",
    "        words = word_tokenize(text)\n",
    "        return ' '.join([self.stemmer.stem(word) for word in words])\n",
    "\n",
    "    def lemmatize_words(self, text):\n",
    "        words = word_tokenize(text)\n",
    "        return ' '.join([self.lemmatizer.lemmatize(word) for word in words])\n",
    "\n",
    "    # Order matters\n",
    "    def preprocess(self, text):\n",
    "        text = self.expand_contractions(text)\n",
    "        text = self.split_hyphenated_words(text)\n",
    "        text = self.add_space_after_parenthesis(text)\n",
    "\n",
    "        #In case we need to analyse sentences, we will need the punctuations\n",
    "        if not self.sentence_analysis:\n",
    "            text = self.remove_punctuation(text)\n",
    "        text = self.to_lowercase(text)\n",
    "        #The stopwords are removed if the users wants to standardise variants.\n",
    "        #If none is selected, the ouput will just perform previous cleaning steps\n",
    "        if self.processing_mode != 'none':\n",
    "            text = self.remove_stopwords(text)\n",
    "\n",
    "        text = self.remove_extra_whitespace(text)\n",
    "\n",
    "        #Select the processing mode for variants\n",
    "        if self.processing_mode == 'lemma':\n",
    "            text = self.lemmatize_words(text)\n",
    "        elif self.processing_mode == 'stem':\n",
    "            text = self.stem_words(text)\n",
    "\n",
    "        return text\n",
    "\n",
    "    #Apply preprocessing steps to daframe and create a column base on the processing mode\n",
    "    def preprocess_dataframe(self, df, column_name):\n",
    "        if not self.sentence_analysis:\n",
    "            if self.processing_mode == 'lemma':\n",
    "                df[f'{column_name}_processed_lemma'] = df[column_name].apply(self.preprocess)\n",
    "            elif self.processing_mode == 'stem':\n",
    "                df[f'{column_name}_processed_stem'] = df[column_name].apply(self.preprocess)\n",
    "            else:  # If 'none', apply preprocessing without lemma or stem\n",
    "                df[f'{column_name}_processed_cleaned'] = df[column_name].apply(self.preprocess)\n",
    "        else: # Add different processed columns for sentences\n",
    "            if self.processing_mode == 'lemma':\n",
    "                df[f'{column_name}_processed_lemma_sent'] = df[column_name].apply(self.preprocess)\n",
    "            elif self.processing_mode == 'stem':\n",
    "                df[f'{column_name}_processed_stem_sent'] = df[column_name].apply(self.preprocess)\n",
    "            else:  # If 'none', apply preprocessing without lemma or stem\n",
    "                df[f'{column_name}_processed_cleaned_sent'] = df[column_name].apply(self.preprocess)\n",
    "        return df\n",
    "\n",
    "#=============================== GET THE EMBEDDINGS\n",
    "\n",
    "def get_embeddings(text, model_name):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    inputs = tokenizer(str(text), return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "    outputs = model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1).detach().to(\"cpu\").numpy()\n",
    "    return embeddings\n",
    "    \n",
    "    \n",
    "#============================== GET THE SKILLS\n",
    "\n",
    "# Define a function to apply the matcher and find skills in the text\n",
    "def find_skills(text):\n",
    "    doc = nlp(text)\n",
    "    matches = matcher(doc)\n",
    "    skills = set()  # To store found skills\n",
    "    for match_id, start, end in matches:\n",
    "        skill = doc[start:end].text\n",
    "        skills.add(skill)\n",
    "    return skills\n",
    "\n",
    "\n",
    "def find_top_skills(job_desc_embedding, skill_embeddings, skills_list, threshold=0.55):\n",
    "    # Ensure that job_desc_embedding is 2D before passing to cosine_similarity\n",
    "    job_desc_embedding = np.expand_dims(job_desc_embedding, axis=0)  # Make it 2D\n",
    "    similarities = cosine_similarity(job_desc_embedding, skill_embeddings).flatten()\n",
    "    \n",
    "    # Find all skills with similarity scores above the threshold\n",
    "    above_threshold_indices = [i for i, score in enumerate(similarities) if score >= threshold]\n",
    "    \n",
    "    # Get the skills and scores for those above the threshold\n",
    "    top_skills = [skills_list[i] for i in above_threshold_indices]\n",
    "    top_scores = [similarities[i] for i in above_threshold_indices]\n",
    "    \n",
    "    # Return both the skills and their similarity scores\n",
    "    return list(zip(top_skills, top_scores))\n",
    "\n",
    "\n",
    "# Function to evaluate a new job description\n",
    "def evaluate_new_job_description(new_job_description, model_name, skill_embeddings, skills_list, ):\n",
    "    # Generate embeddings for the new job description\n",
    "    job_desc_embedding = get_embeddings(new_job_description, model_name)\n",
    "    # Find the top skills using cosine similarity\n",
    "    top_skills_with_scores = find_top_skills(job_desc_embedding, skill_embeddings, skills_list, THRESHOLD)\n",
    "    \n",
    "    return top_skills_with_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619daf25-68ac-486d-87fb-4ef2153648ab",
   "metadata": {},
   "source": [
    "# LOAD FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8a7c74c-16b1-4963-b4dc-7e92b2dc0c99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Load the files\n",
    "\n",
    "with open('../data/job_desc_embeddings_skills.pkl', 'rb') as f:\n",
    "    skill_embeddings = pickle.load(f)\n",
    "\n",
    "with open('../data/job_desc_embeddings.pkl', 'rb') as f:\n",
    "    job_desc_embeddings = pickle.load(f)\n",
    "\n",
    "    \n",
    "# Load patterns from the JSONL file\n",
    "skills_patterns = []\n",
    "with open('../data/jz_skill_patterns.jsonl', 'r') as f:\n",
    "    for line in f:\n",
    "        skills_patterns.append(json.loads(line))\n",
    "        \n",
    "skills_list = load_skills_from_json('../data/skills.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75608df6-23f5-4b82-a6bb-48d4b4a8867d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Add patterns to the matcher\n",
    "for pattern in skills_patterns:\n",
    "    matcher.add(pattern['label'], [pattern['pattern']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1452286-d963-4070-8e52-3999c5a90247",
   "metadata": {},
   "source": [
    "# PREDICT SKILLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "112a25ad-3080-4ed6-a35e-5852ad372c40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example of a new job description\n",
    "new_job_desc = \"We are looking for a data scientist with experience in machine learning and Python and SQL. Knowledge in database and version control and webscrapping.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f5189c9f-e20f-4071-99be-6d5bc85a6d28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_preprocessor = TextPreprocessor(processing_mode='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b473aec9-e97c-4156-94ec-4354da850543",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Preprocess the job description text\n",
    "new_job_cleaned = text_preprocessor.preprocess(new_job_desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ed860140-73e9-42b7-89c6-fcdb55ae0a49",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'we are looking for a data scientist with experience in machine learning and python and sql knowledge in database and version control and webscrapping'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_job_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ef5996a0-304c-460f-aa81-0d1f9facd8b3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'database', 'machine learning', 'python'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skills_matched = find_skills(new_job_cleaned)\n",
    "skills_matched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "205f6457-f000-4c02-8baa-48dc4a420a85",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['database', 'machine learning', 'SQL', 'Python']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skills_matched = list(find_skills(new_job_desc))\n",
    "skills_matched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b590745c-8030-4751-848a-fe30cccabb03",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AutoModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m THRESHOLD \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.6\u001b[39m\n\u001b[1;32m      2\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbert-base-uncased\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModel\u001b[49m\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n\u001b[1;32m      4\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Example of a new job description\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'AutoModel' is not defined"
     ]
    }
   ],
   "source": [
    "THRESHOLD = 0.6\n",
    "model_name = \"bert-base-uncased\"\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "# Evaluate the new job description\n",
    "top_skills_for_new_job = evaluate_new_job_description(new_job_desc, model_name, skill_embeddings, skills_list)\n",
    "\n",
    "print(\"Top skills for the new job description:\", top_skills_for_new_job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25c9aad-fc43-454b-8f90-cb23b660b22a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
