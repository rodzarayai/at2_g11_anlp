# -*- coding: utf-8 -*-
"""NER

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ngit0N8p6kxpaVqtnO-UE2qWgXfu8GgK

# NER Model

In this notebook, we train a NER model to identify organisations and skills based on CVs

## Steps in the notebook


1. Setting the environment
2. Loading and exploring datasets     
3. Defining NER model for entity identification
4. Defining NER model for skills identification
5. Preparing the embedding

## Setting the environment and loading datasets

Install necessary libraries and packages
"""

!pip install spacy
!python -m spacy download en_core_web_sm
!pip install pdfminer.six
!pip install docx2txt
!pip install PyPDF2

# Importing essential libraries and functions
import spacy
nlp = spacy.load('en_core_web_sm')

import pandas as pd
import numpy as np
import re
import nltk
from nltk.corpus import stopwords
from numpy import array
import seaborn as sns
import matplotlib.pyplot as plt
import time
import json
from collections import Counter
from collections import defaultdict
import torch
import docx2txt
import random
import PyPDF2
import os


from tqdm import tqdm
from transformers import pipeline, AutoTokenizer, AutoModel
from subprocess import list2cmdline
from pdfminer.high_level import extract_text
from sklearn.metrics.pairwise import cosine_similarity
from spacy.matcher import Matcher
from tensorflow.keras.preprocessing.text import one_hot, Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Activation, Dropout, Dense, Flatten, GlobalMaxPooling1D, Embedding, Conv1D, LSTM
from tensorflow.keras.layers import Flatten, GlobalMaxPooling1D, Embedding, Conv1D, LSTM
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer

# Mounting google drive

from google.colab import drive
drive.mount('/content/drive')

from google.colab import files
uploaded = files.upload()

from google.colab import files
uploaded = files.upload()

"""## Exploring datasets"""

# Explore resume file

text = pd.read_csv("Resume.csv")

# Dataset exploration

text.shape

text.head(5)

#Explore skills file

data = []
with open("jz_skill_patterns.jsonl", 'r') as f:
    for line in f:
        data.append(json.loads(line))


text2 = pd.DataFrame(data)

text2.shape

text2.head(5)

text2.tail(5)

"""## Defining NER model for entity identification

Parts of this model were developed in reference to: https://stackoverflow.com/questions/72986264/company-name-extraction-with-bert-base-ner-easy-way-to-know-which-words-relate

Certain debugging was done using Claude.
"""

#Defining NER model for entity identification

start = time.time()
model_checkpoint = "xlm-roberta-large-finetuned-conll03-english"
token_classifier = pipeline(
    "token-classification", model=model_checkpoint, aggregation_strategy="simple"
)

def text_extraction(file):
    try:
        df = pd.read_csv(file)
        text = ' '.join(df.astype(str).values.flatten())
        return text.replace('\t', ' ')
    except Exception as e:
        print(f"Error reading CSV file: {e}")
        return None

def preprocess_text(text):
    # Remove HTML tags
    text = re.sub(r'<[^>]+>', ' ', text)
    # Replace newlines and multiple spaces with a single space
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

def extract_organizations(text):
    # Use regex to find potential company names
    company_patterns = [
        r'(?:^|\s)(?:at|for|with)\s+((?:[A-Z][a-z]+\s?)+(?:Inc\.?|LLC|Ltd\.?|Corporation|Corp\.?|Company|Co\.?)?)',
        r'(?:^|\s)((?:[A-Z][a-z]+\s?)+(?:Inc\.?|LLC|Ltd\.?|Corporation|Corp\.?|Company|Co\.?))',
        r'Company Name\s*[－-]\s*([^,]+)'
    ]

    potential_orgs = []
    for pattern in company_patterns:
        potential_orgs.extend(re.findall(pattern, text))

    # Use the model for additional entity recognition
    classifier_results = token_classifier(text)
    model_orgs = [item["word"] for item in classifier_results if item["entity_group"] == "ORG"]

    # Combine results
    all_orgs = list(set(potential_orgs + model_orgs))

    return post_process_orgs(all_orgs)

def post_process_orgs(orgs):
    # Remove duplicates and sort
    orgs = sorted(set(orgs))
    # Remove generic terms misclassified as organizations
    blacklist = {'hr', 'human resources', 'payroll', 'assistant', 'intern', 'coordinator', 'manager', 'company name'}
    orgs = [org for org in orgs if org.lower() not in blacklist]
    return orgs

def analyze_resume(file):
    extracted_text = text_extraction(file)
    if extracted_text is None:
        return

    preprocessed_text = preprocess_text(extracted_text)
    organizations = extract_organizations(preprocessed_text)

    print("Extracted Organizations:")
    for i, org in enumerate(organizations, 1):
        print(f"{i}. {org}")

    print(f"\nTotal unique organizations extracted: {len(organizations)}")

csv_file = "Resume.csv"
analyze_resume(csv_file)

end = time.time()
print(f"\nExecution time: {round((end - start), 2)} seconds")

"""## Defining NER2 for skills identification

"""

#Defining NER model for skills identification

start = time.time()
model_checkpoint = "xlm-roberta-large-finetuned-conll03-english"
token_classifier = pipeline(
    "token-classification", model=model_checkpoint, aggregation_strategy="simple"
)

def text_extraction(file):
    """
    To extract texts from pdf, word, or csv files
    """
    if file.endswith(".pdf"):
        return extract_text(file)
    elif file.endswith(".docx"):
        resume_text = docx2txt.process(file)
        if resume_text:
            return resume_text.replace('\t', ' ')
        return None
    elif file.endswith(".csv"):
        try:
            df = pd.read_csv(file)
            text = ' '.join(df.astype(str).values.flatten())
            return text.replace('\t', ' ')
        except Exception as e:
            print(f"Error reading file: {e}")
            return None
    else:
        return None

def load_custom_entities(jsonl_file):
    custom_entities = []
    with open(jsonl_file, 'r') as file:
        for line in file:
            data = json.loads(line)
            pattern = data.get('pattern', [])
            skill = " ".join([item.get('LOWER', '') for item in pattern]).strip()
            if skill:
                custom_entities.append(skill)
    return custom_entities

def preprocess_text(text):
    text = text.lower()
    text = re.sub(r'[^\w\s]', ' ', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text

def find_skills(text, skills):
    found_skills = []
    for skill in skills:
        if skill in text:
            found_skills.append(skill)
    return found_skills

def custom_entity_extraction(file, custom_entities):
    extracted_text = text_extraction(file)
    if not extracted_text:
        print("No text extracted from the file.")
        return

    preprocessed_text = preprocess_text(extracted_text)

    # Find skills in the preprocessed text
    found_skills = find_skills(preprocessed_text, custom_entities)

    # Use the model for additional entity recognition
    classifier_results = token_classifier(extracted_text)

    # Combine results
    all_entities = set(found_skills + [item['word'].lower() for item in classifier_results if item['word'].lower() in custom_entities])

    # Sort entities
    sorted_entities = sorted(all_entities, key=lambda x: (-len(x), x))

    print("Extracted Skills:")
    for skill in sorted_entities:
        print(f"  {skill}")

    print(f"\nTotal unique skills extracted: {len(sorted_entities)}")

# Load custom entities from JSONL file
jsonl_file = "jz_skill_patterns.jsonl"
custom_entities = load_custom_entities(jsonl_file)

# Apply model
custom_entity_extraction("Resume.csv", custom_entities)

end = time.time()
print(f"\nExecution time: {round((end - start), 2)} seconds")

"""## Defining and testing models with embedding - Organisations

We will now develop the model to be used for our application which will use embeddings to build flexibility and robustness.

We will split our Resume.csv data into a train and validation split so that we can evalaute the performance of our embeddings.

Claude AI was used to identify ways to optimize the code as the original model was taking several hours to process.
"""

# Embedding for entity model

start = time.time()
model_name = "xlm-roberta-large"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name)

def split_data(file, test_size=0.2):
    df = pd.read_csv(file)
    train_df, val_df = train_test_split(df, test_size=test_size, random_state=42)
    return train_df, val_df

def text_extraction(df):
    text = ' '.join(df.astype(str).values.flatten())
    return text.replace('\t', ' ')

def preprocess_text(text):
    text = re.sub(r'<[^>]+>', ' ', text)
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

def get_embedding(text):
    inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=512, padding=True)
    with torch.no_grad():
        outputs = model(**inputs)
    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()

def extract_potential_orgs(text):
    company_patterns = [
        r'(?:^|\s)(?:at|for|with)\s+((?:[A-Z][a-z]+\s?)+(?:Inc\.?|LLC|Ltd\.?|Corporation|Corp\.?)?)',
        r'(?:^|\s)((?:[A-Z][a-z]+\s?)+(?:Inc\.?|LLC|Ltd\.?|Corporation|Corp\.?))',
        r'Company Name\s*[－-]\s*([^,]+)'
    ]

    potential_orgs = []
    for pattern in company_patterns:
        potential_orgs.extend(re.findall(pattern, text))

    return list(set(potential_orgs))

def is_likely_org(org_name, org_embedding, known_org_embeddings, threshold=0.8):
    similarities = cosine_similarity([org_embedding], known_org_embeddings)[0]
    return np.max(similarities) > threshold

def post_process_orgs(orgs):
    orgs = sorted(set(orgs))
    blacklist = {'hr', 'human resources', 'payroll', 'assistant', 'intern', 'coordinator', 'manager', 'company name', 'Zip Code Phone Number Email Address Date Employer Co', 's', 'io', 'wide', 'Youth', 'You',
                 'Year Achievement Award Navy Unit Commendation Navy Meritorious Unit Co', 'Work History Company Name Certifications Inorganic Chemistry Organic Chemistry Materials Science Nanomaterials Unit Operations Fluid Mechanics Thermodynamics Transport Phenomenon Process Co',
                 'Success', 'City', 'Accountant', 'Accounting', 'Accounts', 'Accreditation', 'Accruals', 'Act', 'Action', 'Active Directory', 'Administration', 'Administrators',
                 'Advertising', 'Advising', 'Advisor', 'Assistant', 'Agile', 'Attending daily fit meetings; evaluating the status and functionality of samples based on updates before approving for production. - Creating/updating line sheets',
                 'Business Development', 'City', 'Closing', 'Community', 'Compliance', 'Executive', 'Interest', 'Leadership', 'Microsoft Excel', 'Operations' , 'Qualifications', 'State', 'Substitute Teacher'
                 }
    orgs = [org for org in orgs if org.lower() not in blacklist]
    return orgs

def analyze_resume(df, set_name):
    extracted_text = text_extraction(df)
    preprocessed_text = preprocess_text(extracted_text)
    potential_orgs = extract_potential_orgs(preprocessed_text)

    known_orgs = ["Google", "Microsoft", "Apple", "Amazon", "Facebook", "IBM", "Intel"]
    known_org_embeddings = [get_embedding(org) for org in known_orgs]

    confirmed_orgs = []
    for org in potential_orgs:
        org_embedding = get_embedding(org)
        if is_likely_org(org, org_embedding, known_org_embeddings):
            confirmed_orgs.append(org)

    confirmed_orgs = post_process_orgs(confirmed_orgs)

    print(f"\nExtracted Organizations ({set_name}):")
    for i, org in enumerate(confirmed_orgs, 1):
        print(f"{i}. {org}")

    print(f"\nTotal unique organizations extracted ({set_name}): {len(confirmed_orgs)}")

csv_file = "Resume.csv"
train_df, val_df = split_data(csv_file)

print(f"Training set size: {len(train_df)}")
print(f"Validation set size: {len(val_df)}")

analyze_resume(train_df, "Training Set")
analyze_resume(val_df, "Validation Set")

end = time.time()
print(f"\nExecution time: {round((end - start), 2)} seconds")

"""## Defining and testing models with embedding - Skills"""

#Model with embeddings (1 hour processing time)

def preprocess_text(text):
    text = text.lower()
    text = re.sub(r'\s+', ' ', text)
    text = re.sub(r'[^\w\s]', ' ', text)
    return text.strip()

def load_skill_patterns(jsonl_file):
    skill_patterns = []
    with open(jsonl_file, 'r') as file:
        for line in file:
            try:
                data = json.loads(line)
                pattern = data.get('pattern', [])
                skill = ' '.join([item.get('LOWER', '') for item in pattern if isinstance(item, dict)])
                if skill:
                    skill_patterns.append(skill)
            except json.JSONDecodeError:
                print(f"Error decoding JSON line: {line}")
    return skill_patterns

@torch.no_grad()
def get_embedding(text, tokenizer, model):
    inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=512, padding=True)
    outputs = model(**inputs)
    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()

def extract_skills(resume_embedding, skill_embeddings, skill_patterns, threshold=0.75):
    similarities = cosine_similarity([resume_embedding], skill_embeddings)[0]
    return [skill for skill, sim in zip(skill_patterns, similarities) if sim > threshold]

def process_cv(cv_text, tokenizer, model, skill_patterns, skill_embeddings):
    preprocessed_text = preprocess_text(cv_text)
    resume_embedding = get_embedding(preprocessed_text, tokenizer, model)
    extracted_skills = extract_skills(resume_embedding, skill_embeddings, skill_patterns)

    skill_counts = Counter()
    for skill in extracted_skills:
        count = len(re.findall(r'\b' + re.escape(skill) + r'\b', preprocessed_text, re.IGNORECASE))
        if count > 0:
            skill_counts[skill] = count

    return skill_counts

def process_csv(csv_file, jsonl_file):
    model_name = "distilroberta-base"
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModel.from_pretrained(model_name)

    skill_patterns = load_skill_patterns(jsonl_file)
    skill_embeddings = [get_embedding(skill, tokenizer, model) for skill in skill_patterns]

    df = pd.read_csv(csv_file)

    all_skill_counts = {}

    print("Processing CVs...")
    for _, row in tqdm(df.iterrows(), total=len(df)):
        cv_id = row['ID']
        cv_text = row['Resume_str']

        skill_counts = process_cv(cv_text, tokenizer, model, skill_patterns, skill_embeddings)
        all_skill_counts[cv_id] = skill_counts

    return all_skill_counts

def main():
    csv_file = "Resume.csv"  # Your CSV file with multiple CVs
    jsonl_file = "jz_skill_patterns.jsonl"

    all_skill_counts = process_csv(csv_file, jsonl_file)

    print("\nTop 10 Extracted Skills for each CV:")
    for cv_id, skill_counts in all_skill_counts.items():
        print(f"\nCV ID: {cv_id}")
        for skill, count in skill_counts.most_common(10):
            print(f"  {skill} (Count: {count})")

    # Calculate overall skill frequency across all CVs
    overall_skill_counts = Counter()
    for skill_counts in all_skill_counts.values():
        overall_skill_counts.update(skill_counts)

    print("\nTop 20 Overall Extracted Skills:")
    for skill, count in overall_skill_counts.most_common(20):
        print(f"  {skill} (Total Count: {count})")

    print(f"\nTotal unique skills extracted across all CVs: {len(overall_skill_counts)}")

if __name__ == "__main__":
    main()

from google.colab import files
uploaded = files.upload()

#Model for one pdf CV

def extract_text_from_pdf(pdf_path):
    with open(pdf_path, 'rb') as file:
        reader = PyPDF2.PdfReader(file)
        text = ' '.join(page.extract_text() for page in reader.pages)
    return text

def extract_text_from_docx(docx_path):
    doc = docx.Document(docx_path)
    return ' '.join(paragraph.text for paragraph in doc.paragraphs)

def extract_text_from_csv(csv_path):
    with open(csv_path, 'r', encoding='utf-8') as file:
        reader = csv.reader(file)
        return ' '.join(' '.join(row) for row in reader)

def extract_text(file_path):
    _, ext = os.path.splitext(file_path)
    if ext.lower() == '.pdf':
        return extract_text_from_pdf(file_path)
    elif ext.lower() == '.docx':
        return extract_text_from_docx(file_path)
    elif ext.lower() == '.csv':
        return extract_text_from_csv(file_path)
    else:
        raise ValueError(f"Unsupported file type: {ext}")

def preprocess_text(text):
    text = text.lower()
    text = re.sub(r'\s+', ' ', text)
    text = re.sub(r'[^\w\s]', ' ', text)
    return text.strip()

def load_skill_patterns(jsonl_file):
    skill_patterns = []
    with open(jsonl_file, 'r') as file:
        for line in file:
            try:
                data = json.loads(line)
                pattern = data.get('pattern', [])
                skill = ' '.join([item.get('LOWER', '') for item in pattern if isinstance(item, dict)])
                if skill:
                    skill_patterns.append(skill)
            except json.JSONDecodeError:
                print(f"Error decoding JSON line: {line}")
    return skill_patterns

@torch.no_grad()
def get_embedding(text, tokenizer, model):
    inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=512, padding=True)
    outputs = model(**inputs)
    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()

def extract_skills(resume_embedding, skill_embeddings, skill_patterns, threshold=0.75):
    similarities = cosine_similarity([resume_embedding], skill_embeddings)[0]
    return [skill for skill, sim in zip(skill_patterns, similarities) if sim > threshold]

def process_resume(file_path, jsonl_file):
    model_name = "distilroberta-base"
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModel.from_pretrained(model_name)

    skill_patterns = load_skill_patterns(jsonl_file)

    with tqdm(total=100, desc="Processing resume") as pbar:
        resume_text = extract_text(file_path)
        pbar.update(20)

        preprocessed_text = preprocess_text(resume_text)
        pbar.update(20)

        resume_embedding = get_embedding(preprocessed_text, tokenizer, model)
        pbar.update(20)

        skill_embeddings = [get_embedding(skill, tokenizer, model) for skill in skill_patterns]
        pbar.update(20)

        extracted_skills = extract_skills(resume_embedding, skill_embeddings, skill_patterns)
        pbar.update(20)

    skill_counts = Counter()
    for skill in extracted_skills:
        count = len(re.findall(r'\b' + re.escape(skill) + r'\b', preprocessed_text, re.IGNORECASE))
        if count > 0:
            skill_counts[skill] = count

    return skill_counts

def main():
    resume_file = "10554236.pdf"  # Replace with your resume file path
    jsonl_file = "jz_skill_patterns.jsonl"

    try:
        skill_counts = process_resume(resume_file, jsonl_file)

        print("\nTop 20 Extracted Skills:")
        for skill, count in skill_counts.most_common(20):
            print(f"  {skill} (Count: {count})")

        print(f"\nTotal unique skills extracted: {len(skill_counts)}")
    except Exception as e:
        print(f"Error processing {resume_file}: {str(e)}")

if __name__ == "__main__":
    main()